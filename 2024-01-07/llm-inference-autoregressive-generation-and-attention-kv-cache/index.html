


<!DOCTYPE html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="dark"
  data-auto-appearance="true"
><head>
  <meta charset="utf-8" />
  
    <meta http-equiv="content-language" content="en" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="rgb(255,255,255)" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title>LLM Inference: Autoregressive Generation and Attention KV Cache &middot; Better Tomorrow with Computer Science</title>
    <meta name="title" content="LLM Inference: Autoregressive Generation and Attention KV Cache &middot; Better Tomorrow with Computer Science" />
  
  <meta name="description" content="This post explains the basic of LLM inference, mainly focusing on differences from training LLM.
Autreogressive Text Generation # Unlike training, where tokens are parallelized and trained, inference generates tokens one by one. Therefore, to create a full sentence, several forward pass should be executed (# tokens times). The following video from HuggingFace illustrates how it works.
Autoregressive token generation. Source: HuggingFace
Before generating the first token, LLM first puts all input tokens to generate context." />
  
  
  
  <link rel="canonical" href="/2024-01-07/llm-inference-autoregressive-generation-and-attention-kv-cache/" />
  
  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.10f51640c62455dc3b2a174cda772e2fee409808982122a17db5129978d396e5a9dec1b325b2c50f7e5f7dbc7d8c62bdcb89f86574ad3d8359acad96fb66e674.css"
    integrity="sha512-EPUWQMYkVdw7KhdM2ncuL&#43;5AmAiYISKhfbUSmXjTluWp3sGzJbLFD35ffbx9jGK9y4n4ZXStPYNZrK2W&#43;2bmdA=="
  />
  
  
  <script type="text/javascript" src="/js/appearance.min.1e44157c1e51b46341ce2c94716dd3dd1ef30c28e7d1c9f3b9db80877bfe72bc66b00d8a662f317840016acbed93c5f18c3d9268c8b957021ee74d0b04adf6c5.js" integrity="sha512-HkQVfB5RtGNBziyUcW3T3R7zDCjn0cnzuduAh3v&#43;crxmsA2KZi8xeEABasvtk8XxjD2SaMi5VwIe500LBK32xQ=="></script>
  
  
    
    
  
  
  
    
    <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.9947db70e5b36e741e53f8531bd734741b3fc7a688f7b5d749fc5433c48efc252f47bdf44e471570a550fc051b87712799014040e6349b38d64448b94e84e45a.js" integrity="sha512-mUfbcOWzbnQeU/hTG9c0dBs/x6aI97XXSfxUM8SO/CUvR730TkcVcKVQ/AUbh3EnmQFAQOY0mzjWREi5ToTkWg==" data-copy="Copy" data-copied="Copied"></script>
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  <meta property="og:title" content="LLM Inference: Autoregressive Generation and Attention KV Cache" />
<meta property="og:description" content="This post explains the basic of LLM inference, mainly focusing on differences from training LLM.
Autreogressive Text Generation # Unlike training, where tokens are parallelized and trained, inference generates tokens one by one. Therefore, to create a full sentence, several forward pass should be executed (# tokens times). The following video from HuggingFace illustrates how it works.
Autoregressive token generation. Source: HuggingFace
Before generating the first token, LLM first puts all input tokens to generate context." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2024-01-07/llm-inference-autoregressive-generation-and-attention-kv-cache/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-07T18:20:00-05:00" />
<meta property="article:modified_time" content="2024-01-07T18:20:00-05:00" /><meta property="og:site_name" content="Better Tomorrow with Computer Science" />

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="LLM Inference: Autoregressive Generation and Attention KV Cache"/>
<meta name="twitter:description" content="This post explains the basic of LLM inference, mainly focusing on differences from training LLM.
Autreogressive Text Generation # Unlike training, where tokens are parallelized and trained, inference generates tokens one by one. Therefore, to create a full sentence, several forward pass should be executed (# tokens times). The following video from HuggingFace illustrates how it works.
Autoregressive token generation. Source: HuggingFace
Before generating the first token, LLM first puts all input tokens to generate context."/>

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "LLM Inference: Autoregressive Generation and Attention KV Cache",
    "headline": "LLM Inference: Autoregressive Generation and Attention KV Cache",
    
    "abstract": "This post explains the basic of LLM inference, mainly focusing on differences from training LLM.\nAutreogressive Text Generation # Unlike training, where tokens are parallelized and trained, inference generates tokens one by one. Therefore, to create a full sentence, several forward pass should be executed (# tokens times). The following video from HuggingFace illustrates how it works.\nAutoregressive token generation. Source: HuggingFace\nBefore generating the first token, LLM first puts all input tokens to generate context.",
    "inLanguage": "en",
    "url" : "\/2024-01-07\/llm-inference-autoregressive-generation-and-attention-kv-cache\/",
    "author" : {
      "@type": "Person",
      "name": "Insu Jang"
    },
    "copyrightYear": "2024",
    "dateCreated": "2024-01-07T18:20:00-05:00",
    "datePublished": "2024-01-07T18:20:00-05:00",
    
    "dateModified": "2024-01-07T18:20:00-05:00",
    
    "keywords": ["dl","inference","attention"],
    
    "mainEntityOfPage": "true",
    "wordCount": "518"
  }]
  </script>


  
  <meta name="author" content="Insu Jang" />
  
    
      <link href="mailto:insujang@umich.edu" rel="me" />
    
      <link href="https://www.linkedin.com/in/insujang" rel="me" />
    
      <link href="https://github.com/insujang" rel="me" />
    
      <link href="https://scholar.google.com/citations?user=U6I8Y98AAAAJ" rel="me" />
    
  
  
  
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,400;0,700;1,400;1,700&display=swap"
    rel="stylesheet">





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body, {
        delimiters: [
            {left: '$$', right: '$$' , display: true},
            {left: '$', right: '$' , display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
        ] });"></script>































































  
  
  
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1T9WZMKHVB"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-1T9WZMKHVB', { 'anonymize_ip': false });
}
</script>



  
  
</head>
<body
    class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400"
          >&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral print:hidden sm:py-10">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >Better Tomorrow with Computer Science</a
  >

      

    </div>
    
    
      <ul class="flex list-none flex-col ltr:text-right rtl:text-left sm:flex-row">
        
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/#about"
                title=""
                >About</a
              >
            </li>
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/research/"
                title=""
                >Research</a
              >
            </li>
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/posts/"
                title=""
                >Posts</a
              >
            </li>
          
        
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        LLM Inference: Autoregressive Generation and Attention KV Cache
      </h1>
      <div class="mt-8 mb-12 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
    
  

  

  

  

  

  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2024-01-07 18:20:00 -0500 -0500">Jan 7, 2024</time>
    

    
    
  </div>

  
  
    <div class="my-1 text-xs text-neutral-500 dark:text-neutral-400 ">
      
        
          
            <a
              href="/tags/dl/"
              class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >dl</a
            >
          
            <a
              href="/tags/inference/"
              class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >inference</a
            >
          
            <a
              href="/tags/attention/"
              class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >attention</a
            >
          
        
      
    </div>
  


      </div>
      
    </header>
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
        <div class="order-first px-0 lg:order-last lg:max-w-xs ltr:lg:pl-8 rtl:lg:pr-8">
          <div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10">
            <details open class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"
  >
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"
  >
    <nav id="TableOfContents">
  <ul>
    <li><a href="#autreogressive-text-generation">Autreogressive Text Generation</a>
      <ul>
        <li><a href="#transformer-attention-layer">Transformer Attention Layer</a></li>
        <li><a href="#key-value-cache">Key-Value Cache</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</details>

          </div>
        </div>
      
      <div class="min-w-0 min-h-0 max-w-prose grow">
        <p>This post explains the basic of LLM inference, mainly focusing on differences from training LLM.</p>
<h1 id="autreogressive-text-generation" class="relative group">Autreogressive Text Generation <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#autreogressive-text-generation" aria-label="Anchor">#</a></span></h1>
<p>Unlike training, where tokens are parallelized and trained, inference generates tokens one by one. Therefore, to create a full sentence, several forward pass should be executed (# tokens times).
The following video from HuggingFace illustrates how it works.</p>
<blockquote>
<figure>
<video autoplay src="/assets/images/240107/text_generation.mov"></video>
</figure>
<p>Autoregressive token generation. Source: <a href="https://huggingface.co/docs/transformers/v4.33.3/llm_tutorial"   target="_blank">HuggingFace</a></p>
</blockquote>
<p>Before generating the first token, LLM first puts all input tokens to generate context. This stage is called <em>prefill</em> stage.
Because input tokens are fixed, tokens in the prefill stage can be consumed in parallel.
After prefill is done, <em>decode</em> stage begins; where LLM goes through iterations to create tokens one by one.</p>
<h2 id="transformer-attention-layer" class="relative group">Transformer Attention Layer <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#transformer-attention-layer" aria-label="Anchor">#</a></span></h2>
<blockquote>
<p>To understand the detailed explanation, I recommend to read <a href="https://jalammar.github.io/illustrated-gpt2/"   target="_blank">JAy Alammar&rsquo;s blog post</a>, and <a href="https://www.youtube.com/watch?v=80bIUggRJf4&amp;t=323s"   target="_blank">a Youtube Video</a></p>
</blockquote>
<p>Now let&rsquo;s see why token generation should be done autoregressively. This is because how attention is calculated.
A decoder-only transformer based models, such as GPT, looks like this:</p>
<p>



  
  
  
    <figure>
      <img class="my-0 rounded-md" src="/assets/images/220730/transformer.png" alt="" />
      
    </figure>
  

</p>
<p>$$ \text{Attention(Q, K, V)} = \text{softmax}\ \frac{QK^T}{\sqrt{d_k}} V $$</p>
<p>where query (Q), key (K), and value (V) vectors are calculated as:</p>
<p>$$ Q = W^qx, K = W^kx, V = W^vx$$
Here, key represents the relationship between all previous tokens and the current token, and value is an accumulated sum over the previous context.</p>
<p>Therefore, we have in total 5 matmul operations to calculate self attention:</p>
<ol>
<li>$Q = W^qx$</li>
<li>$K = W^kx$</li>
<li>$V = W^vx$</li>
<li>$QK^T$</li>
<li>$\text{softmax}(4)V$.</li>
</ol>
<h2 id="key-value-cache" class="relative group">Key-Value Cache <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#key-value-cache" aria-label="Anchor">#</a></span></h2>
<p>Because key and value matrices are for maintaining context, matrices that are generated in previous iterations can be cached and reused, instead of calculating the entire K/V from scratch. It is called <em>key-value (KV) cache</em>.
Assuming model dimension as D and number of attention heads as H, dimensions per head is D/H. Then the dimension of key/value matrix is <code>[D, N, D/H]</code> for a single batch, where N is the number of tokens in the context.</p>
<p>As an example, say we have 7 tokens in the input:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">OPTForCausalLM</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;gpt2&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&#34;left&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;gpt2&#34;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="s2">&#34;A red jacket compliments any outfit.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># input[&#34;input_ids&#34;].shape: torch.Size([1, 7])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="nb">input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</span></span></code></pre></div><p>In the first iteration, we have the following shape for query, key, and value:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">64</span><span class="p">]))</span>
</span></span></code></pre></div><p>where D=12 and D/H=64.</p>
<blockquote>
<p>Code location: <a href="https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/gpt2/modeling_gpt2.py#L312"   target="_blank">https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/gpt2/modeling_gpt2.py</a></p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_attn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></div><p>After prefill is done, <code>key</code> and <code>value</code> only have parameters for one token:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">]))</span>
</span></span></code></pre></div><p>then concatenated to the past kv:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">layer_past</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">past_key</span><span class="p">,</span> <span class="n">past_value</span> <span class="o">=</span> <span class="n">layer_past</span>
</span></span><span class="line"><span class="cl">    <span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">past_key</span><span class="p">,</span> <span class="n">key</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">past_value</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></div><pre tabindex="0"><code class="language-pyton" data-lang="pyton">&gt;&gt;&gt; key.shape, value.shape
(torch.Size([1, 12, 8, 64]), torch.Size([1, 12, 8, 64]))
</code></pre><p>which saves huge amount of computation for calculating 2 and 3.
Note that this does not save computation for 4 and 5; as context is getting longer, K and V size is larger, and matmul takes longer to compute.</p>

      </div>
    </section>
    <footer class="pt-8 max-w-prose print:hidden">
      
  <div class="flex">
    
      
      
        
        <img
          class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4"
          width="96"
          height="96"
          alt="Insu Jang"
          src="/profile_hufec33d24ce71c5c22cc5620410f7e1d1_126481_192x192_fill_q75_box_smart1.jpg"
        />
      
    
    <div class="place-self-center">
      
        <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
          Author
        </div>
        <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
          Insu Jang
        </div>
      
      
      <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          href="mailto:insujang@umich.edu"
          target="_blank"
          aria-label="Email"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://www.linkedin.com/in/insujang"
          target="_blank"
          aria-label="Linkedin"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://github.com/insujang"
          target="_blank"
          aria-label="Github"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://scholar.google.com/citations?user=U6I8Y98AAAAJ"
          target="_blank"
          aria-label="Google-Scholar"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <!-- Uploaded to: SVG Repo, www.svgrepo.com, Transformed by: SVG Repo Mixer Tools -->
<svg fill="currentColor" width="800px" height="800px" viewBox="0 0 24 24" role="img" xmlns="http://www.w3.org/2000/svg"><title>Google Scholar icon</title><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"/></svg>
  </span>

</a
        >
      
    
  </div>

</div>
    </div>
  </div>


      

      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="group flex" href="/2023-04-22/torch-fx-transformation-and-pipeline-parallelism/">
              <span
                class="mr-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
              <span
                class="ml-2 text-neutral-700 transition-transform group-hover:translate-x-[2px] group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Torch FX Transformation and Pipeline Parallelism</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2023-04-22 11:13:00 -0400 -0400">Apr 22, 2023</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="group flex text-right" href="/2024-01-07/llm-inference-continuous-batching-and-pagedattention/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >LLM Inference: Continuous Batching and PagedAttention</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2024-01-07 20:20:00 -0500 -0500">Jan 7, 2024</time>
                  
                </span>
              </span>
              <span
                class="ml-2 text-neutral-700 transition-transform group-hover:translate-x-[2px] group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        
          <div
            class="pointer-events-none absolute top-[100vh] bottom-0 w-12 ltr:right-0 rtl:left-0"
          >
            <a
              href="#the-top"
              class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
              aria-label="Scroll to top"
              title="Scroll to top"
            >
              &uarr;
            </a>
          </div>
        
      </main><footer class="py-10 print:hidden">
  
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2025
            Insu Jang
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://git.io/hugo-congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    
    
      <div
        class="ltr:mr-14 rtl:ml-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
      >
        <button id="appearance-switcher" type="button">
          <div
            class="flex items-center justify-center w-12 h-12 dark:hidden"
            title="Switch to dark appearance"
          >
            

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


          </div>
          <div
            class="items-center justify-center hidden w-12 h-12 dark:flex"
            title="Switch to light appearance"
          >
            

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


          </div>
        </button>
      </div>
    
  </div>
  
  
</footer>

    </div>
  </body>
</html>
