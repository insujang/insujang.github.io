<!DOCTYPE html>
<html lang='en' ><meta charset="utf-8">
<meta name="viewport" content="width=device-width">


<title>Analyzing Parallelization of Attention | Better Tomorrow with Computer Science</title>
<link rel="stylesheet" href="/css/eureka.min.css">
<script defer src="/js/eureka.min.js"></script>

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.8.6/css/academicons.min.css"
   crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js"
   crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js"
   integrity="sha256-uNYoXefWRqv&#43;PsIF/OflNmwtKM4lStn9yrz2gVl6ymo="  crossorigin></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
   integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3&#43;Aro6EYUG4&#43;cU&#43;KJWu/X"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" 
  integrity="sha384-g7c&#43;Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI&#43;sEnkvrMWph2EDg4"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
   integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC&#43;Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>
<link rel="preconnect" href="https://www.google-analytics.com" crossorigin>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158110335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'UA-158110335-1');
</script>


<link rel="icon" type="image/png" sizes="32x32" href="/umich_hu6c99b92144fcbc4e30752e6c6d9a0d50_18545_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="/umich_hu6c99b92144fcbc4e30752e6c6d9a0d50_18545_180x180_fill_box_center_3.png">

<meta name="description"
  content="We exploit the inherent parallelism in the multi-head attention operation to partition the self-attention block (shown in Figure 5b).">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Posts",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"Analyzing Parallelization of Attention",
      "item":"/2022-08-03/analyzing-parallelization-of-attention/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/2022-08-03/analyzing-parallelization-of-attention/"
    },
    "headline": "Analyzing Parallelization of Attention | Better Tomorrow with Computer Science","datePublished": "2022-08-03T14:30:00-04:00",
    "dateModified": "2022-08-03T14:30:00-04:00",
    "wordCount":  953 ,
    "publisher": {
        "@type": "Person",
        "name": "Insu Jang",
        "logo": {
            "@type": "ImageObject",
            "url": "/umich.png"
        }
        },
    "description": "We exploit the inherent parallelism in the multi-head attention operation to partition the self-attention block (shown in Figure 5b)."
}
</script><meta property="og:title" content="Analyzing Parallelization of Attention | Better Tomorrow with Computer Science" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/umich.png">


<meta property="og:url" content="/2022-08-03/analyzing-parallelization-of-attention/" />




<meta property="og:description" content="We exploit the inherent parallelism in the multi-head attention operation to partition the self-attention block (shown in Figure 5b)." />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="Better Tomorrow with Computer Science" />






<meta property="article:published_time" content="2022-08-03T14:30:00-04:00" />


<meta property="article:modified_time" content="2022-08-03T14:30:00-04:00" />



<meta property="article:section" content="posts" />


<meta property="article:tag" content="dl" />





<meta property="og:see_also" content="/2022-07-30/analysis-of-transformer-model/" />

<meta property="og:see_also" content="/2022-06-11/parallelism-in-distributed-deep-learning/" />



<body class="flex flex-col min-h-screen">
  <header class="fixed flex items-center w-full min-h-16 pl-scrollbar z-50 bg-secondary-bg shadow-sm">
    <div class="w-full max-w-screen-xl mx-auto"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if ((storageColorScheme == 'Auto' && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap p-4">
    <a href="/" class="mr-6 text-primary-text font-bold">Better Tomorrow with Computer Science</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about"
                class="block mt-4 md:inline-block md:mt-0  hover:text-eureka mr-4">About</a>
            <a href="/posts/"
                class="block mt-4 md:inline-block md:mt-0  hover:text-eureka mr-4">Posts</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-sun"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka">Light</span>
                    <span class="px-4 py-1 hover:text-eureka">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == 'Auto') {
        element.firstElementChild.classList.remove('fa-sun')
        element.firstElementChild.setAttribute("data-icon", 'adjust')
        element.firstElementChild.classList.add('fa-adjust')
        document.addEventListener('DOMContentLoaded', () => {
            switchMode('Auto')
        })
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-sun')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }
    
    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script></div>
  </header>
  <main class="flex-grow pt-16">
      <div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto">


<div class="lg:pt-12"></div>
<div
    class="col-span-2 lg:col-start-2 lg:col-span-6 bg-secondary-bg rounded px-6 py-8">
    <h1 class="font-bold text-3xl text-primary-text">Analyzing Parallelization of Attention</h1>
    <div class="mr-6 my-2">
    <span>Aug 3, 2022</span>
</div>




    
    
    

    <div class="content">
        <blockquote>
<p><em>We exploit the inherent parallelism in the multi-head attention operation to partition the self-attention block (shown in Figure 5b).</em>
<em>The key (K), Query (Q), and value (V) matrices can be partitioned in a column-parallel fashion. The output linear layer can then directly operate on the partitioned output of the attention operation (weight matrix partitioned across rows).</em></p>
<p>Deepak Narayanan et al, <strong><a href="https://dl.acm.org/doi/abs/10.1145/3458817.3476209?casa_token=b3NnsI5AxpkAAAAA:p_WojF9259pFLqMXh-fG1bKEIU9vxzs-Qu5xtJFfXwF7RmHMi6emr1pP1yaUtkXYPEbZkxJiBZNfKA">Efficient large-scale language model training on GPU clusters using megatron-LM</a></strong>, SC'21</p>
</blockquote>
<p>This post analyzes how we can parallelize multi-head attentions, which is a type of tensor-parallelism, but more efficiently without heavy communication like traditional intra-layer parallelisms have, which ZeRO <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> stated so in Section VII-C: <em>MP incurs significantly higher communication compared to ZeRO-DP</em>.</p>
<blockquote>
<p>This post in inspired from the following posts <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
</blockquote>
<figure><img src="/assets/images/220730/transformer.png"
         alt="image"/><figcaption>
            <p>Transformer architecture.</p>
        </figcaption>
</figure>

<p>In the transformer architecture, multiple attention heads exist together in each transformer layer, i.e. 16 heads in GPT-2 medium.
They are independent and linear projections (matmul) in multi-head self attention can be calculated altogether, or independently.</p>
<p>For consistency with <a href="/2022-07-30/analysis-of-transformer-model/">the previous post</a>, I use the following variable names:</p>
<ul>
<li>$a$: number of attention heads. (GPT-2 medium: 16)</li>
<li>$b$: number of microbatch. (variable hyperparameter)</li>
<li>$h$: hidden dimension size. (GPT-2 medium: 1024)</li>
<li>$L$: number of transformer layers. (GPT-2 medium: 24)</li>
<li>$s$: sequence length. (GPT-2 medium: 1024)</li>
<li>$v$: vocabulary size. (GPT-2 medium: 50,257)</li>
</ul>
<p>Layer dimension:</p>
<ul>
<li><strong>Input</strong>: [$s, h, b$]</li>
<li><strong>$W^Q, W^K, W^V$</strong>: [$h, h$] each.</li>
<li><strong>Q, K, V</strong>: [$s, h, b$] each. Calculated from $Y=XW^Y$, where $X$ is input and $Y$ is among Q, K, V.</li>
<li><strong>$attention(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$)</strong>: [$s, h, b$]</li>
</ul>
<h1 id="parallelizing-q-k-and-v">Parallelizing Q, K, and V</h1>
<p>There are $a$ attention heads in a transformer layer, however, they are calculated and used as a single matrix altogether for efficiency in practical computation.
To be specific, although Q, K, and V are stored in contiguous buffer of [$s, h, b$], each Q, K, and V has the dimension of [$s, \frac{h}{a}, b$] (where $\frac{h}{a}$ is called <em>query size</em> and is always 64 for any GPT-2 configuration: 768/12 (GPT-2) == 1024/16 (GPT-2 medium) == 1280/20 (GPT-2 large) == 1600/25 (GPT-2 XL) == 64). Buffers are reshaped to [$s, \frac{h}{a}, a, b$] and used for multi-head attention calculation.</p>
<blockquote>
<p>Refer to the 3D Bert model illustration from <a href="https://peltarion.com/blog/data-science/illustration-3d-bert">here</a> for understanding how it looks like.</p>
</blockquote>
<p>The word from Megatron-LM <em>&ldquo;K, Q, and V matrices can be partitioned in a column-parallel fashion&rdquo;</em> means that, we can split linear projection for Q, K, and V calculation into multiple linear projections in a column-parallel manner:</p>
<p>From: [$s, h$] * [$h, h$] = [$s, h$] $\rightarrow$ To: $a$ independent linear [$s, h$] * [$h, \frac{h}{a}$] = $a$[$s, \frac{h}{a}$]</p>
<figure><img src="/assets/images/220803/query_linear1.png"
         alt="image"/><figcaption>
            <p>Query (Q) linear projection. Only showed 1 batch.</p>
        </figcaption>
</figure>

<p>This single linear projection can be split into $a (==3)$ independent linear projections:
If we say the three linear projections are distributed, no communication is needed except for the input between GPUs.</p>
<figure><img src="/assets/images/220803/query_linear2.png"
         alt="image"/><figcaption>
            <p>Query (Q) split linear projections. Only showed 1 batch.</p>
        </figcaption>
</figure>

<h1 id="parallelizing-attention-calculation">Parallelizing Attention Calculation</h1>
<p>$attention(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</p>
<p>Attention calculation can also be done with concatenated Q, K, and V to calculate multi-head attentions at once, or independently calculate each attention with split Q, K, and V. But note, that Q, K, and V are not used as concatenated for matrix multiplication; each $Q_i$, $K_i$, and $V_i$ should be used independently for each attention calculation. To be specific, it is not a simple matrix multiplication of [$s, h$] $\times$ [$h, s$] $\times$ [$s, h$], but $a \times$ [$s, \frac{h}{a}$] $\times$ [$\frac{h}{s}, s$] $\times$ [$s, h$].</p>
<figure><img src="/assets/images/220803/attention_calc.png"
         alt="image"/><figcaption>
            <p>Attention calculation can be parallelized. Note that normalization is not included in illustration.</p>
        </figcaption>
</figure>

<p>Megatron-LM uses 3D matrix multiplication to calculate attention <a href="https://github.com/NVIDIA/Megatron-LM/blob/3f4e71df3c81ae289e434c863466b658eaab37de/megatron/model/transformer.py#L242">[src]</a>:</p>
<pre><code class="language-python">class CoreAttention(MegatronModule):
    ...
    def forward(...):
        # preallocting input tensor: [b * n, sq, sk]
        matmul_input_buffer = get_global_memory_buffer().get_tensor(
            (output_size[0]*output_size[1], output_size[2], output_size[3]),
            query_layer.dtype, &quot;mpu&quot;)

        # Raw attention scores. [b * n, sq, sk]
        matmul_result = torch.baddbmm(
            matmul_input_buffer,
            query_layer.transpose(0, 1),   # [b * np, sq, hn]
            key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]
            beta=0.0, alpha=(1.0/self.norm_factor))
</code></pre>
<pre><code class="language-python">torch.baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -&gt; Tensor
</code></pre>
<p><code>torch.baddbmm</code> performs a batch matrix-matrix product (or matrix multiplication with <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics">broadcasting</a>); so if batch1 is $(b \times n \times m)$ and batch2 is $(b \times m \times p)$, the input and output dimension is $(b \times n \times p)$ (first dimension $b$ is broadcasted to $(n \times m) \times (m \times p)$ matrix multiplication).</p>
<p>Therefore, by simply adjusting the first dimension to specify how many attentions should be computed together, attention calculation can easily be parallelized. In the example above, if we have 3 attention heads ($a=3$) and 3 GPUs, we can distribute one attention calculation to each GPU by setting the first dimension to 1.</p>
<h1 id="parallelizing-feed-forward-network-mlp">Parallelizing Feed Forward Network (MLP)</h1>
<p>MLP layer includes two linear projections and GeLU; it can be expressed as follows (definition from the Attention paper <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>):</p>
<p>$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$</p>
<p>where the dimension of $x$ is [$s, h$], $W_1$ is [$h, 4h$], $W_2$ is [$4h, h$].</p>
<figure><img src="/assets/images/220803/mlp_projection.png"
         alt="image"/><figcaption>
            <p>Linear projections in MLP or feed forward network. If we use column-parallelization for the first linear projection, the distribution cannot be used for parallelizing the second linear projection as it is.</p>
        </figcaption>
</figure>

<p>There are two ways of parallelization: split input in a row-parallel manner (left), or split weight in a column-parallel manner (right). Considering that we need to perform the second linear projection with the output of the first linear projection after GeLU applied (element-wise operation), applying row-parallelization is reasonable, since it does not require communication across nodes if they already have a replicated weights. If we apply column-parallelization for the first linear projection, data should be exchanged in nodes (either every node has a replication or at least a row).</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://ieeexplore.ieee.org/abstract/document/9355301">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853">Transformer Explained Visually (Part 3): Multi-head Attention, Deep Dive</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is All you Need</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>
    
    <div class="my-4">
    
    <a href="/tags/dl/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 mr-2 hover:text-eureka">#dl</a>
    
</div>
    
    
    
    
    
    
    
<div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t">
    <div>
        
    </div>
    <div class="md:text-right mt-4 md:mt-0">
        
        <span class="block font-bold">Next</span>
        <a href="/2022-07-30/analysis-of-transformer-model/" class="block">Analysis of Transformer Model</a>
        
    </div>
</div>

    

<div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "insujang" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


</div>




<div
    class="col-span-2 lg:col-start-2 lg:col-span-6 bg-secondary-bg rounded p-6">
    <h2 class="text-lg font-semibold mb-4">See Also</h2>
    <div class="content">
        
        <a href="/2022-07-30/analysis-of-transformer-model/">Analysis of Transformer Model</a>
        <br />
        
        <a href="/2022-06-11/parallelism-in-distributed-deep-learning/">Parallelism in Distributed Deep Learning</a>
        <br />
        
    </div>
</div>

</div>
<script>
    document.addEventListener('DOMContentLoaded', ()=>{
        hljs.initHighlightingOnLoad();
    })
</script>

      </div>
    
  </main>
  <footer class="pl-scrollbar">
    <div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2017 - 2021 Insu Jang &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
  </footer>
</body>

</html>