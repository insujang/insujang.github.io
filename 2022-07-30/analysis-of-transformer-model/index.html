


<!DOCTYPE html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="dark"
  data-auto-appearance="true"
><head>
  <meta charset="utf-8" />
  
    <meta http-equiv="content-language" content="en" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="rgb(255,255,255)" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title>Analysis of Transformer Model &middot; Better Tomorrow with Computer Science</title>
    <meta name="title" content="Analysis of Transformer Model &middot; Better Tomorrow with Computer Science" />
  
  <meta name="description" content="This post analyzes transformer models, specifically memory and computation overhead.
Many transformer based models just explain themselves as a model with X-B parameters; I wanted to break it down and look into the model structure how they are stored and used in actual computing hardware.
 Many illustrations and analysis are based on the following papers 1.
 Transformer-based Model # Since Google announed attention and transformer models in 2017 2, MLP and image classification models are rapidly adopting transformer layers." />
  
  
  
  <link rel="canonical" href="/2022-07-30/analysis-of-transformer-model/" />
  
  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.10f51640c62455dc3b2a174cda772e2fee409808982122a17db5129978d396e5a9dec1b325b2c50f7e5f7dbc7d8c62bdcb89f86574ad3d8359acad96fb66e674.css"
    integrity="sha512-EPUWQMYkVdw7KhdM2ncuL&#43;5AmAiYISKhfbUSmXjTluWp3sGzJbLFD35ffbx9jGK9y4n4ZXStPYNZrK2W&#43;2bmdA=="
  />
  
  
  <script type="text/javascript" src="/js/appearance.min.75869c865625ed3d10fe38f2274fa90938094d28b518ee2088f544a29fe6b826626bae550302adcbde61e83ba341bdd928a250d644367723ce01e798033098c5.js" integrity="sha512-dYachlYl7T0Q/jjyJ0&#43;pCTgJTSi1GO4giPVEop/muCZia65VAwKty95h6DujQb3ZKKJQ1kQ2dyPOAeeYAzCYxQ=="></script>
  
  
    
    
  
  
  
    
    <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.6441f8726be4ada95c479c17d34ce0c4f9f734611947904121e8c317136b5a33ca848f1a18723cc0e68b65872b89b5da63293ec3deb8650739be41c518e6f292.js" integrity="sha512-ZEH4cmvkralcR5wX00zgxPn3NGEZR5BBIejDFxNrWjPKhI8aGHI8wOaLZYcribXaYyk&#43;w964ZQc5vkHFGObykg==" data-copy="Copy" data-copied="Copied"></script>
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  <meta property="og:title" content="Analysis of Transformer Model" />
<meta property="og:description" content="This post analyzes transformer models, specifically memory and computation overhead.
Many transformer based models just explain themselves as a model with X-B parameters; I wanted to break it down and look into the model structure how they are stored and used in actual computing hardware.
 Many illustrations and analysis are based on the following papers 1.
 Transformer-based Model # Since Google announed attention and transformer models in 2017 2, MLP and image classification models are rapidly adopting transformer layers." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2022-07-30/analysis-of-transformer-model/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-30T23:48:00-04:00" />
<meta property="article:modified_time" content="2022-07-30T23:48:00-04:00" /><meta property="og:site_name" content="Better Tomorrow with Computer Science" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Analysis of Transformer Model"/>
<meta name="twitter:description" content="This post analyzes transformer models, specifically memory and computation overhead.
Many transformer based models just explain themselves as a model with X-B parameters; I wanted to break it down and look into the model structure how they are stored and used in actual computing hardware.
 Many illustrations and analysis are based on the following papers 1.
 Transformer-based Model # Since Google announed attention and transformer models in 2017 2, MLP and image classification models are rapidly adopting transformer layers."/>

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "Analysis of Transformer Model",
    "headline": "Analysis of Transformer Model",
    
    "abstract": "This post analyzes transformer models, specifically memory and computation overhead.\nMany transformer based models just explain themselves as a model with X-B parameters; I wanted to break it down and look into the model structure how they are stored and used in actual computing hardware.\n Many illustrations and analysis are based on the following papers 1.\n Transformer-based Model # Since Google announed attention and transformer models in 2017 2, MLP and image classification models are rapidly adopting transformer layers.",
    "inLanguage": "en",
    "url" : "\/2022-07-30\/analysis-of-transformer-model\/",
    "author" : {
      "@type": "Person",
      "name": "Insu Jang"
    },
    "copyrightYear": "2022",
    "dateCreated": "2022-07-30T23:48:00-04:00",
    "datePublished": "2022-07-30T23:48:00-04:00",
    
    "dateModified": "2022-07-30T23:48:00-04:00",
    
    "keywords": ["dl"],
    
    "mainEntityOfPage": "true",
    "wordCount": "1635"
  }]
  </script>


  
  <meta name="author" content="Insu Jang" />
  
    
      <link href="mailto:insujang@umich.edu" rel="me" />
    
      <link href="https://www.linkedin.com/in/insujang" rel="me" />
    
      <link href="https://github.com/insujang" rel="me" />
    
      <link href="https://scholar.google.com/citations?user=U6I8Y98AAAAJ" rel="me" />
    
  
  
  
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,400;0,700;1,400;1,700&display=swap"
    rel="stylesheet">





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body, {
        delimiters: [
            {left: '$$', right: '$$' , display: true},
            {left: '$', right: '$' , display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
        ] });"></script>































































  
  
  
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-158110335-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


  
  
</head>
<body
    class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400"
          >&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral print:hidden sm:py-10">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >Better Tomorrow with Computer Science</a
  >

      

    </div>
    
    
      <ul class="flex list-none flex-col ltr:text-right rtl:text-left sm:flex-row">
        
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/#about"
                title=""
                >About</a
              >
            </li>
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/research/"
                title=""
                >Research</a
              >
            </li>
          
            <li class="mb-1 sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
                href="/posts/"
                title=""
                >Posts</a
              >
            </li>
          
        
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        Analysis of Transformer Model
      </h1>
      <div class="mt-8 mb-12 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
    
  

  

  

  

  

  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2022-07-30 23:48:00 -0400 EDT">Jul 30, 2022</time>
    

    
    
  </div>

  
  
    <div class="my-1 text-xs text-neutral-500 dark:text-neutral-400 ">
      
        
          
            <a
              href="/tags/dl/"
              class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400"
              >dl</a
            >
          
        
      
    </div>
  


      </div>
      
    </header>
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
        <div class="order-first px-0 lg:order-last lg:max-w-xs ltr:lg:pl-8 rtl:lg:pr-8">
          <div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10">
            <details open class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"
  >
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"
  >
    <nav id="TableOfContents">
  <ul>
    <li><a href="#transformer-based-model">Transformer-based Model</a></li>
    <li><a href="#memory-consumption">Memory Consumption</a>
      <ul>
        <li><a href="#model-parameters-or-weights">Model Parameters (or Weights)</a></li>
        <li><a href="#activation">Activation</a></li>
        <li><a href="#gradient-and-optimizer-state">Gradient and Optimizer State</a></li>
        <li><a href="#total-memory-consumption">Total Memory Consumption</a></li>
      </ul>
    </li>
    <li><a href="#layer-dimension">Layer Dimension</a></li>
    <li><a href="#flops-for-computing">FLOPs for Computing</a></li>
  </ul>
</nav>
  </div>
</details>

          </div>
        </div>
      
      <div class="min-w-0 min-h-0 max-w-prose grow">
        <p>This post analyzes transformer models, specifically memory and computation overhead.</p>
<p>Many transformer based models just explain themselves as a model with X-B parameters; I wanted to break it down and look into the model structure how they are stored and used in actual computing hardware.</p>
<blockquote>
<p>Many illustrations and analysis are based on the following papers <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
</blockquote>
<h1 id="transformer-based-model" class="relative group">Transformer-based Model <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#transformer-based-model" aria-label="Anchor">#</a></span></h1>
<p>Since Google announed attention and transformer models in 2017 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, MLP and image classification models are rapidly adopting transformer layers.
Redrawed from the paper <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, basic transformer architecture looks like this:</p>

  
  
  
  
  
  
    
    
    
      <figure>
        <img class="my-0 rounded-md" src="/assets/images/220730/transformer.png" alt="image" />
        <figcaption>Transformer architecture.</figcaption>
      </figure>
    
  


<blockquote>
<p>MLP is the same with feed-forward layer in GPT. For more information about GPT specific illustration, refer to <a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html"   target="_blank">this post</a>.</p>
</blockquote>
<p>For example, GPT-2 XL has 48 transformer layers, hence has 1.5B parameters <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. But how this number comes out? Which layers in the figure have parameters? And how many memory is actually required to train this model? This is the goal of the post.</p>
<p>Following the paper <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, I used the following variable names in the rest of the post:</p>
<ul>
<li>$a$: number of attention heads. (GPT-2 medium: 16)</li>
<li>$b$: number of microbatch. (variable hyperparameter.)</li>
<li>$h$: hidden dimension size. (GPT-2 medium: 1024)</li>
<li>$L$: number of transformer layers. (GPT-2 medium: 24)</li>
<li>$s$: sequence length. (GPT-2 medium: 1024)</li>
<li>$v$: vocabulary size. (GPT-2 medium: 50,257)</li>
</ul>
<p>The other variables (pipeline parallel size and tensor parallel size) are not used here.</p>
<h1 id="memory-consumption" class="relative group">Memory Consumption <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#memory-consumption" aria-label="Anchor">#</a></span></h1>
<p>Memory consumption in training can be divided into the following four types:</p>
<ul>
<li>model: model parameters (weights).</li>
<li>activation: after doing forward pass, the result of computation is stored and reused in backpropagation to calculate weight gradient (Can be discarded and recomputed via activation recomputation, I ignore this technique here).</li>
<li>gradient: calculated during backpropagation. Used to update model parameters.</li>
<li>optimizer state: e.g. Adam optimizer uses internal states to keep gradient momentum. Optimizer state includes such optimizer-specific data.</li>
</ul>
<h2 id="model-parameters-or-weights" class="relative group">Model Parameters (or Weights) <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#model-parameters-or-weights" aria-label="Anchor">#</a></span></h2>
<blockquote>
<p>Model parameters are stored in FP16 in mixed precision, so each parameter takes 2 bytes.</p>
</blockquote>
<p>From the figure, all linear projection layers have their parameters. I omitted biases and normalization for layers.</p>
<ul>
<li>To calculate Q, K, and V, $W^Q, W^K, W^V$ are used. Each $W$ has dimension of [$h$, $h$], Total number of parameters is $3h^2L$. Note that this linear projection calculates all Q, K, and V for $a$ attention heads at once.</li>
<li>The result of multiple attentions are concatenated and fed into linear projection (and dropout) before going into MLP. Total number of parameters for linear here is $h^2L$ ([$h$, $h$]). <!-- See [3D illustrated transformer layer](https://peltarion.com/blog/data-science/illustration-3d-bert) why this linear projection has such dimension. --></li>
<li>MLP accepts the outputs from attention ($h$), casts them to $4h$ (here, the linear projection has parameters of [$h, 4h$]), applies GeLU, casts them back to $h$ (this linear projectionh as parameters of [$4h, h$]). In total, we have $8h^2L$ parameters.</li>
<li>Word embeddings and positional encoding: The input of the entire model is calculated from word embeddings and position encoding. Numbers of parameters are $vh$ and $sh$, respectively. See <a href="https://jalammar.github.io/illustrated-gpt2/"   target="_blank">Illustrated GPT-2</a> for more details.</li>
</ul>
<p>Total # parameters: $Lh^2(3 + 1 + 8) + vh + sh$. If we focus on each layer, it is $12h^2$.</p>
<p>For instance, GPT-2 medium: $24 * 12 * 1024^2 + 50257 * 1024 + 1024^2 = 354,501,632$ (~345M parameters. The calculation is bigger than stated by OpenAI, like <a href="https://jalammar.github.io/illustrated-gpt2/#part-3-beyond-language-modeling"   target="_blank">the author of the post</a> said.)</p>
<h2 id="activation" class="relative group">Activation <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#activation" aria-label="Anchor">#</a></span></h2>
<p>The paper <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> mostly focus on reducing memory usage of activation. I borrow their analysis here (page 4).</p>
<blockquote>
<ul>
<li>Unlike model parameters, activation is proportional to the number of microbatch ($b$).</li>
<li>Their activation analysis is per layer. Need to multiply by $L$.</li>
<li>Their calculation is to get the amount of memory usage in <em>bytes</em>. For mask elements, only a byte per element is required, while others activations are stored in FP16 (2 bytes per element).</li>
</ul>
</blockquote>
<p>(1) for attention</p>
<ul>
<li><strong>Q, K, and V matrix multiplies</strong>: only need to store shared input $2sbh$.</li>
<li><strong>$QK^T$ matrix multiply</strong>: requires stotage of both Q and K with total size $4sbh$.</li>
<li><strong>Softmax</strong>: softmax output with size $2as^2b$ is required for back-propagation.</li>
<li><strong>Softmax dropout</strong>: only a mask with size $as^2b$ is needed. (Note that they stated only a single byte per element is required for mask).</li>
<li><strong>Attention over Values</strong>: need to store the dropout output ($2as^2b$) and V ($2sbh$).</li>
<li><strong>Post-atention linear and dropout</strong>: required to store activations ($2sbh$ and $sbh$, respectively).</li>
</ul>
<p>(2) for MLP</p>
<ul>
<li><strong>Linear, GeLU, and Dropout</strong>: Two linear layers and GeLU non-linarlity needs <em>their inputs</em> with $2sbh, 8sbh, 8sbh$, respectively, for back-propagation. dropout stores its mask with size $sbh$. Total: $19sbh$</li>
</ul>
<p>(3) for <strong>Layer Norm</strong>: each layer norm stores its input with size $2sbh$ (total: $4sbh$).</p>
<p>Activation memory per layer: $sbh(34+5\frac{as}{h})$ or $34sbh+5as^2b$</p>
<p>For instance, GPT-2 medium with 8 microbatches: $1024 * 8 * 1024 * (34 + 5 * \frac{16 * 1024}{1024})$ = 912MB per layer. Considering that <strong>the entire</strong> model would only take 676MB, activation memory is so huge.</p>
<h2 id="gradient-and-optimizer-state" class="relative group">Gradient and Optimizer State <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#gradient-and-optimizer-state" aria-label="Anchor">#</a></span></h2>
<blockquote>
<p>Gradient is stored in FP16 as well, but the optimizer keeps an FP32 copy of the parameters and FP32 copy of all the other optimizer states, according to ZeRO <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
</blockquote>
<p>The dimension of gradients is the same with that of the corresponding parameters. Hence, $12h^2$ per layer, or $12Lh^2+vh+sh$ for the entire model.</p>
<p>For optimizer state, Adam, for example, holds an FP32 copy of the parameters, momentum, and variance (assuming the number of parameter is $P$, memory requirement is $4P, 4P, 4P$, respectively).</p>
<p>Total gradient memory consumption: $2 * (12Lh^2 + (s+v)h)$, and for optimizer state: $12 * (12Lh^2 + (s+v)h)$.</p>
<p>For instance, GPT-2 medium: 709,003,264 bytes (676MB) for gradient, 4,254,019,584 bytes (3.96GB) for optimizer state.</p>
<h2 id="total-memory-consumption" class="relative group">Total Memory Consumption <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#total-memory-consumption" aria-label="Anchor">#</a></span></h2>
<p>If not considering activation recomputation, peak memory consumption for GPT-2 medium with 8 microbatch would be roughly:</p>
<p>676MB (model parameters) + 21.375GB (activation) + 676MB (gradient) + 3.96GB (optimizer state) ~= 26.66GB</p>
<pre tabindex="0"><code>node00               Sun Jul 31 06:44:34 2022  510.60.02
[0] NVIDIA A40       | 36'C,   0 % | 29621 / 46068 MB |
</code></pre><p>Note that actual running includes more parameters (biases), input data, kernel, internal fragmented allocated but unused buffer, etc. Tested with <a href="https://github.com/nvidia/megatron-lm"   target="_blank">Megatron-LM code</a> (FP16, Adam, 8 microbatch size, GPT-2 medium).</p>
<h1 id="layer-dimension" class="relative group">Layer Dimension <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#layer-dimension" aria-label="Anchor">#</a></span></h1>
<ul>
<li>Model input: [$s, v, b$]. Multiplied by word embedding ([$v, h$]) (and add the position encoding [$s, h$]) to create a transformer layer input [$s, h, b$].</li>
<li>Transformer layer input: [$s, h, b$].</li>
<li>Q, K, and V: layer input [$s, h, b$] multiplied by weight [$h, h$] for each batch, respectively. each has [$s, h, b$]. Note that they are broken down to $a$ Q, K, and Vs, each of which has [$s, \frac{h}{a}, b$] ($\frac{h}{a}$ is called a <em>query size</em> and always 64 for all GPT-2 configuration).</li>
<li>In self-attention, $QK^T$ has [$s, s, b$]. There are $a$ $QK^T$, so in total number of elements is $as^2b$.</li>
<li>Softmax and dropout do not change dimension; same to that of $QK^T$: [$s, s, b$].</li>
<li>The dimension of the attention output after multiplying V ([$s, h, b$]) on $softmax(QK^T)$ ($[s, s, b]$) is [$s, h, b$].</li>
<li>Post attention linear projection multiplies its weight [$h, h, b$] without dimension change; the output dimension is [$s, h, b$].</li>
<li>In MLP, linear projection casts the hidden size of the self attention output from $h$ to $4h$ by multiplying its weight of [$h, 4h, b$]. The output dimension is [$s, 4h, b$].</li>
<li>GeLU is a element-wise operation, so the same with the input: [$s, 4h, b$].</li>
<li>The next linear reduces the input height from $4h$ to $h$ by multiplying its weight of [$4h, h, b$]. The output dimension is [$s, h, b$].</li>
<li>Layer normalization does not change the dimension, which makes the layer output dimension of [$s, h, b$].</li>
</ul>
<h1 id="flops-for-computing" class="relative group">FLOPs for Computing <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#flops-for-computing" aria-label="Anchor">#</a></span></h1>
<p>The paper for activation optimization <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> also analyzed FLOPs calculation in Appendix A. Following their analysis, I also only consider the matrix multiplications (GEMMs: General Matrix Multiply).</p>
<blockquote>
<p>I don&rsquo;t know why, but the paper authors <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> multiply every number of operations by 2. Here my calculations are half of theirs.</p>
</blockquote>
<p>(1) for the attention block, main contributors to operations are:</p>
<ol>
<li><strong>key, query, and value transformation</strong>: $3sbh^2$ ([$s, h$] * [$h, h$] * b each)</li>
<li><strong>attention matrix computation ($\frac{QK^T}{\sqrt{d_k}}$)</strong>: $s^2hb$ ([$s, \frac{h}{a}$] * [$\frac{h}{a}, s$] * b per attention head, and there are $a$ heads)</li>
<li><strong>attention over values (multiply attention matrix by V to calculate $attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$)</strong>: $s^2bh$ ([$s, s$] * [$s, \frac{h}{a}$] * b per attention head, and there are $a$ heads)</li>
<li><strong>post-attention linear projection</strong>: $sbh^2$ ([$s, h$] * [$h, h$] * b. Note that post-attention linear projection is done after concatenating $a$ attention heads into a single matrix of [$s, h$])</li>
</ol>
<!-- Note that 1, 2, 3 can be broken down to $a$ computations since multi-head attentions are independent to each other. -->
<p><strong>Total computation in attention</strong>: $4sbh^2 + 2s^2bh$</p>
<p>(2) for MLP (feed-forward):</p>
<ul>
<li><strong>linear projection casting hidden size $h$ to $4h$ and back to $h$</strong>: $8sbh^2$ ([$s, h$] * [$h, 4h$] * b and [$s, 4h$] * [$4h, h$] * b)</li>
</ul>
<p>For each transformer layer with (1)+(2): $12sbh^2+2s^2bh$ for forward pass, and double the number of FLOPs for back-propagation (to calculate the gradients with respect to both input and weight tensors).</p>
<p><strong>Total FLOPs per layer</strong>: $3b*(12sh^2+2s^2h)$.</p>
<p>For instance, GPT-2 medium with 8 microbatches: 3 * 8 * 15,032,385,536 = 360,777,252,864 FLOPs (336 GFLOPs) per layer per iteration.
Considering the theoretical peak performance for a NVIDIA A40 GPU is 37.42 TFLOPS/s (non-tensor cores. Note peak fp16 tensor flops for A40 is 149.7 TFLOPS/s <a href="https://images.nvidia.com/content/Solutions/data-center/a40/nvidia-a40-datasheet.pdf"   target="_blank">[datasheet]</a>), executing FP+BP for 24 layers in GPT-2 medium would take ~210ms. Of course, actual computing requires data copy across memory hierarcy, and peak performance cannot be achieved because tensor cores are not always used, CUDA kernel launch delay, data transfer, etc. Actual performance in the test system shows:</p>
<pre tabindex="0"><code>iteration      100/     100 | consumed samples:          800 | elapsed time per iteration (ms): 646.3
time (ms) | forward-compute: 255.92 | backward-compute: 280.90 | ...
</code></pre><section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://arxiv.org/abs/2205.05198"   target="_blank">Arxiv: Reducing Activation Recomputation in Large Transformer Models</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://proceedings.neurips.cc/paper/7181-attention-is-all-you-need"   target="_blank">Attention is All you Need</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://huggingface.co/transformers/v2.2.0/pretrained_models.html"   target="_blank">HuggingFace: Pretrained Model</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://ieeexplore.ieee.org/abstract/document/9355301"   target="_blank">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
    </section>
    <footer class="pt-8 max-w-prose print:hidden">
      
  <div class="flex">
    
      
      
        
        <img
          class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4"
          width="96"
          height="96"
          alt="Insu Jang"
          src="/profile_hufec33d24ce71c5c22cc5620410f7e1d1_126481_192x192_fill_q75_box_smart1.jpg"
        />
      
    
    <div class="place-self-center">
      
        <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
          Author
        </div>
        <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
          Insu Jang
        </div>
      
      
      <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          href="mailto:insujang@umich.edu"
          target="_blank"
          aria-label="Email"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://www.linkedin.com/in/insujang"
          target="_blank"
          aria-label="Linkedin"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://github.com/insujang"
          target="_blank"
          aria-label="Github"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>

</a
        >
      
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://scholar.google.com/citations?user=U6I8Y98AAAAJ"
          target="_blank"
          aria-label="Google-Scholar"
          rel="me noopener noreferrer"
          >

  <span class="relative inline-block align-text-bottom icon">
    <!-- Uploaded to: SVG Repo, www.svgrepo.com, Transformed by: SVG Repo Mixer Tools -->
<svg fill="currentColor" width="800px" height="800px" viewBox="0 0 24 24" role="img" xmlns="http://www.w3.org/2000/svg"><title>Google Scholar icon</title><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"/></svg>
  </span>

</a
        >
      
    
  </div>

</div>
    </div>
  </div>


      

      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="group flex" href="/2022-06-11/parallelism-in-distributed-deep-learning/">
              <span
                class="mr-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
              <span
                class="ml-2 text-neutral-700 transition-transform group-hover:translate-x-[2px] group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Parallelism in Distributed Deep Learning</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2022-06-11 19:33:00 -0400 EDT">Jun 11, 2022</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="group flex text-right" href="/2022-08-03/analyzing-parallelization-of-attention/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Analyzing Parallelization of Attention</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2022-08-03 14:30:00 -0400 EDT">Aug 3, 2022</time>
                  
                </span>
              </span>
              <span
                class="ml-2 text-neutral-700 transition-transform group-hover:translate-x-[2px] group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        
          <div
            class="pointer-events-none absolute top-[100vh] bottom-0 w-12 ltr:right-0 rtl:left-0"
          >
            <a
              href="#the-top"
              class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
              aria-label="Scroll to top"
              title="Scroll to top"
            >
              &uarr;
            </a>
          </div>
        
      </main><footer class="py-10 print:hidden">
  
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2024
            Insu Jang
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://git.io/hugo-congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    
    
      <div
        class="ltr:mr-14 rtl:ml-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
      >
        <button id="appearance-switcher" type="button">
          <div
            class="flex items-center justify-center w-12 h-12 dark:hidden"
            title="Switch to dark appearance"
          >
            

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


          </div>
          <div
            class="items-center justify-center hidden w-12 h-12 dark:flex"
            title="Switch to light appearance"
          >
            

  <span class="relative inline-block align-text-bottom icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


          </div>
        </button>
      </div>
    
  </div>
  
  
</footer>

    </div>
  </body>
</html>
