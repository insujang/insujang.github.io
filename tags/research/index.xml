<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>research on Better Tomorrow with Computer Science</title>
    <link>/tags/research/</link>
    <description>Recent content in research on Better Tomorrow with Computer Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Nov 2019 23:07:00 +0900</lastBuildDate><atom:link href="/tags/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Interactions between cri-o and conmon in Kubernetes</title>
      <link>/2019-11-18/interactions-between-crio-and-conmon/</link>
      <pubDate>Mon, 18 Nov 2019 23:07:00 +0900</pubDate>
      
      <guid>/2019-11-18/interactions-between-crio-and-conmon/</guid>
      <description>cri-o #    cri-o is a lightweight container runtime framework for Kubernetes. After introducing Open Container Initiative (OCI) container standard, Red Hat implemented cri-o to support the OCI standard and optimize performances by getting rid of unuseful features from Docker for Kubernetes; hence it is lightweight and for Kubernetes.
   {: width=&amp;ldquo;1000px&amp;rdquo;} cri-o Archituecture. It manages containers under the supervison of Kubelet, a node agent of Kubernetes.</description>
    </item>
    
    <item>
      <title>Kubernetes</title>
      <link>/2019-11-07/kubernetes/</link>
      <pubDate>Thu, 07 Nov 2019 11:17:00 +0900</pubDate>
      
      <guid>/2019-11-07/kubernetes/</guid>
      <description>Kubernetes # Kubernetes is is an container-based cluster orchestration tool, originally implemented by Google. It manages containerized workloads and services in clusters.
 Kubernetes is really an orchestration tool?
Kubernetes does not call itself as an orchestration system, due to its different behaviors from the technical definition of &amp;ldquo;orchestration&amp;rdquo;.
 Orchestration (from [Wikipedia])
Orchestration is the automated configuration, coordination, and management of computer systems and software.
  Orchestration (from [bmc])</description>
    </item>
    
    <item>
      <title>Container Runtime</title>
      <link>/2019-10-31/container-runtime/</link>
      <pubDate>Thu, 31 Oct 2019 21:36:00 +0900</pubDate>
      
      <guid>/2019-10-31/container-runtime/</guid>
      <description>From Docker to Kubernetes, these days container solutions are emerging.
   {: .center-image width=&amp;ldquo;1000px&amp;rdquo;} Why Docker? source: https://www.docker.com. They clear state that Docker is a Container Runtime.
In the era of Docker the term &amp;ldquo;Container runtime&amp;rdquo; was quite clear; the software that runs and manages containers. but as the internal architecture is being complicated and various container frameworks are introduced, this definition becomes unclear.
Here are very clear explanations what is container runtime exactly, written by Ian Lewis:</description>
    </item>
    
    <item>
      <title>Open Container Initiative (OCI) Standard, Image Spec</title>
      <link>/2019-10-10/open-container-initiative-image-spec/</link>
      <pubDate>Thu, 10 Oct 2019 20:35:00 +0900</pubDate>
      
      <guid>/2019-10-10/open-container-initiative-image-spec/</guid>
      <description>The Open Container Initiative (OCI) standard is an open standard for Linux containers. As born in 2013, Docker has been a de-facto standard of Linux container framework, but the OCI standard was born for a need of open standard, based on the Docker manifest. As the standard is based on Docker manifest, its specifications and structures are very similar to Dockers&#39;, enabling providing compatibilities between Docker and OCI-based container frameworks.</description>
    </item>
    
    <item>
      <title>Implementing a New Custom Netlink Family Protocol</title>
      <link>/2019-02-07/implementing-a-new-custom-netlink-family-protocol/</link>
      <pubDate>Thu, 07 Feb 2019 17:00:00 +0900</pubDate>
      
      <guid>/2019-02-07/implementing-a-new-custom-netlink-family-protocol/</guid>
      <description>Netlink Protocol # Netlink is a communication protocol between kernel and userspace. Unlike ioctl(), netlink is based on socket, which enables notification from the kernel to userspace. With ioctl(), the kernel can only send a response regarding to a user request. With netlink socket, however, user processes can be blocked via blocking functions such as recv() to receive any messages from the kernel.
#include &amp;lt;asm/types.h&amp;gt;#include &amp;lt;sys/socket.h&amp;gt;#include &amp;lt;linux/netlink.h&amp;gt; netlink_socket = socket (AF_NETLINK, socket_type, netlink_family); There are some predefined famous netlink protocol family: for instance, NETLINK_ROUTE for routing and link updates, NETLINK_KOBJECT_UEVENT for device events, and so on.</description>
    </item>
    
    <item>
      <title>udev: Function Flow for KOBJECT_UEVENT kernel group message </title>
      <link>/2018-11-28/udev-function-flow-for-kobject_uevent-kernel-group-message/</link>
      <pubDate>Wed, 28 Nov 2018 14:03:15 +0900</pubDate>
      
      <guid>/2018-11-28/udev-function-flow-for-kobject_uevent-kernel-group-message/</guid>
      <description>Identifying the device #    {: .center-image} [source]
When a USB device is inserted to system, the very first initialization function to be started is drivers/usb/core/usb.c:usb_init(), written in [here]. The USB root hub driver (i.e. hcd) initiates the USB device initialization, the USB core takes the control and initializes an actual device structure struct usb_device.
linux/include/linux/usb.h struct usb_device { int devnum; char devpath[16]; ... struct usb_device *parent; struct usb_bus *bus; struct usb_host_endpoint ep0; struct device dev; .</description>
    </item>
    
    <item>
      <title>udev: Device Manager for the Linux Kernel in Userspace</title>
      <link>/2018-11-27/udev-device-manager-for-the-linux-kernel-in-userspace/</link>
      <pubDate>Tue, 27 Nov 2018 10:05:14 +0900</pubDate>
      
      <guid>/2018-11-27/udev-device-manager-for-the-linux-kernel-in-userspace/</guid>
      <description>What is udev? #  udev (userspace /dev) is a device manager for the Linux kernel. As the successor of devfsd and hotplug, udev primaily manages device nodes in the /dev directory. At the same time, udev also handls all user space events raised when hardware devices are added into the system or removed from it, including firmware loading as reuqired by certain devices.
https://en.wikipedia.org/wiki/Udev
 udev first appeared at Linux kernel version 2.</description>
    </item>
    
    <item>
      <title>systemd Boot Process</title>
      <link>/2018-11-22/systemd-boot-process/</link>
      <pubDate>Thu, 22 Nov 2018 13:49:40 +0900</pubDate>
      
      <guid>/2018-11-22/systemd-boot-process/</guid>
      <description>What is systemd? #  systemd is a suite of basic building blocks for a Linux system. It provides a system and service manager that runs as PID 1 and starts the rest of the system.
https://www.freedesktop.org/wiki/Software/systemd
 systemd is now the init process running as PID 1 as indicated above. /sbin/init was the actual init process of Linux (also known as System V init boot system), it is now replaced with /usr/lib/systemd in many Linux distributions.</description>
    </item>
    
    <item>
      <title>Intel SGX Sealing</title>
      <link>/2017-10-09/intel-sgx-sealing/</link>
      <pubDate>Mon, 09 Oct 2017 15:26:34 +0900</pubDate>
      
      <guid>/2017-10-09/intel-sgx-sealing/</guid>
      <description>There is a few information related to sealing, even no detailed explanation in the paper: Intel SGX explaned. All in this post are from Intel, with a little thought of mine.
Sealing # Sealing is a service that Intel provides with Intel SGX technology for secure data saving.
Intel SGX provides protections data only if it is in the enclave, part of main memory. Therefore, when the enclave process exits, the enclave will be destroyed and any data that is secured whithin the enclave will be lost.</description>
    </item>
    
    <item>
      <title>NVMeDirect</title>
      <link>/2017-09-06/nvmedirect/</link>
      <pubDate>Wed, 06 Sep 2017 20:40:37 +0900</pubDate>
      
      <guid>/2017-09-06/nvmedirect/</guid>
      <description>NVMeDirect Overview # NVMeDirect is a software framework that is used to directly access to a NVMe SSD from a user space application.
In the existing system, applications should access to a storage through several software I/O stacks.   {: .center-image width=&amp;ldquo;800px&amp;rdquo;}
As the storage media is getting faster, overheads by the software stack takes larger portion of I/O time. Hence, this software I/O stack is being reduced as shown in the above figure.</description>
    </item>
    
    <item>
      <title>Enabling AtomicOps in VFIO PCI Passthrough</title>
      <link>/2017-08-16/enabling-atomicops-in-vfio-pci-passthrough/</link>
      <pubDate>Wed, 16 Aug 2017 16:48:47 +0900</pubDate>
      
      <guid>/2017-08-16/enabling-atomicops-in-vfio-pci-passthrough/</guid>
      <description>I tried to use AMD&amp;rsquo;s GPU for research, with the software stack called AMD ROCm. AMD ROCm is a AMG discrete GPU compatible Heterogeneous System Architecure (HSA) framework.
HSA is unique properties, different from OpenCL or CUDA: user mode queuing mode.   {: .center-image}
Instead of traditional driver centrl models, in HSA, user applications are responsible to create a request packet to the command queue heading to the GPU directly.</description>
    </item>
    
    <item>
      <title>GPU Resource Management</title>
      <link>/2017-06-23/gpu-resource-management/</link>
      <pubDate>Fri, 23 Jun 2017 10:13:09 +0900</pubDate>
      
      <guid>/2017-06-23/gpu-resource-management/</guid>
      <description>This post is a study of the paper Operating Systems Challenges for GPU Resource Management (International Workshop on Operating Systems Platforms for Embedded Real-Time Applications, 2011), and Implementing Open-Source CUDA Runtime (Programming Symposium, 2013).
   {: .center-image width=&amp;ldquo;800px&amp;rdquo;}
The GPU channel is an interface that bridges across the CPU and the GPU contexts, especially when sending GPU commands from the CPU to the GPU.
GPU channel is the only way to send GPU commands to the GPU.</description>
    </item>
    
    <item>
      <title>Implementing and Using Custom Intel SGX Trusted Library 2</title>
      <link>/2017-06-01/implementing-and-using-custom-intel-sgx-trusted-library-2/</link>
      <pubDate>Thu, 01 Jun 2017 19:58:54 +0900</pubDate>
      
      <guid>/2017-06-01/implementing-and-using-custom-intel-sgx-trusted-library-2/</guid>
      <description>In the previous post, I showed how to link a trusted function that can be called insdie the enclave.
However, Intel SGX provides a way to import EDL to make a library have an ECALL. The post from Intel is [here].
1. Implementing a trusted SGX library # As we do in the previous post, make a trusted library.
   {: .center-image}
So our simple trusted SGX library has a function named ecall_testlib_sample.</description>
    </item>
    
    <item>
      <title>Implementing and Using Custom Intel SGX Trusted Library</title>
      <link>/2017-05-31/implementing-and-using-custom-intel-sgx-trusted-library/</link>
      <pubDate>Wed, 31 May 2017 18:42:44 +0900</pubDate>
      
      <guid>/2017-05-31/implementing-and-using-custom-intel-sgx-trusted-library/</guid>
      <description>Intel SGX Trusted Library # Trusted libraries are libraries that are linked to a SGX program, and used inside an enclave. Hence, it should follow SGX enclave restrictions to be used.
According to Intel SGX SDK document, restrictions are as follow.
 Trusted libraries are static libraries that linked with the enclave binary. This functions/objects can only be used from within the enclave. (=ECALL cannot be implemented in a library) We should not link the enclave with any trusted library including C/C++ standard libraries.</description>
    </item>
    
    <item>
      <title>Usermode Helper API</title>
      <link>/2017-05-10/usermode-helper-api/</link>
      <pubDate>Wed, 10 May 2017 01:58:06 +0900</pubDate>
      
      <guid>/2017-05-10/usermode-helper-api/</guid>
      <description>We already know that fork() and exec() are system calls for making a new process from user space.
However, system calls cannot be called in kernel space. Then how to execute a process from kernel space?
Usermode Helper API is for creating a user mode process from kernel space.
Data structure that is used for the API is struct subprocess_info.
/linux/include/kmod.h struct subprocess_info { struct work_struct work; struct completion* complete; const char* path; char** argv; char** envp; int wait; int retval; int (*init)(struct subprocess_info* info, struct cred* new); void (*cleanup)(struct subprocess_info* info); void* data; }; Simple example from the reference:</description>
    </item>
    
    <item>
      <title>CPU Affinity</title>
      <link>/2017-05-09/cpu-affinity/</link>
      <pubDate>Tue, 09 May 2017 21:56:25 +0900</pubDate>
      
      <guid>/2017-05-09/cpu-affinity/</guid>
      <description>CPU affinity, also called CPU pinning, enables the binding of a process or a thread to a specific CPU core, or CPU cores.
The following function is provided as a standard library to set affinity. [reference]
#include &amp;lt;sched.h&amp;gt; int sched_setaffinity(pid_t pid, size_t cpusetsize, cpu_set_t *mask); int sched_getaffinity(pid_t pid, size_t cpusetsize, cpu_set_t *mask); On success, schedu_setaffinity() and sched_getaffinity() return 0. On error, -1 is returned, and errno is set appropriately.
cpu_set_t can be set by defined macros as follows.</description>
    </item>
    
    <item>
      <title>Calling Kernel Module&#39;s Function from The Other Kernel Module</title>
      <link>/2017-05-03/calling-kernel-modules-function-from-the-other-kernel-module/</link>
      <pubDate>Wed, 03 May 2017 15:18:17 +0900</pubDate>
      
      <guid>/2017-05-03/calling-kernel-modules-function-from-the-other-kernel-module/</guid>
      <description>I made two kernel modules, one of which calls a function of the other module, but it kept saying me that WARNING: &amp;quot;&amp;lt;function_name&amp;gt;&amp;quot; undefined!. Even though I exported the function, there actually is another step that I should follow.
References: http://stackoverflow.com/a/9499893
What I did before finding the reference was to export the target function.
/kernel1/functions.h void function1(void); /kernel1/functions.c #include &amp;lt;linux/module.h&amp;gt;void function1(void){}; EXPORT_SYMBOL(function1); However, the other kernel module (say it kernel2) does not know where function1() exists.</description>
    </item>
    
    <item>
      <title>Analyzing Gdev</title>
      <link>/2017-04-27/analyzing-gdev/</link>
      <pubDate>Thu, 27 Apr 2017 22:35:30 +0900</pubDate>
      
      <guid>/2017-04-27/analyzing-gdev/</guid>
      <description>Gdev # Gdev is an open-source CUDA software, containing device drivers, CUDA runtimes, CUDA/PTX compilers, and so on.
You can download it from [here].
Detail implementations are described in the other paper that the author wrote, Implementing Open-Source CUDA Runtime. (link)
Internal Implementation #    {: .center-image}
 Implementation of Gdev  Gdev uses the existing open-source NVIDIA device driver, Nouveau. It also supports NVIDIA proprietary drivers and pscnv as well, pscnv is not maintained and NVIDIA driver is not an open-source.</description>
    </item>
    
    <item>
      <title>Introduction to VFIO</title>
      <link>/2017-04-27/introduction-to-vfio/</link>
      <pubDate>Thu, 27 Apr 2017 13:14:24 +0900</pubDate>
      
      <guid>/2017-04-27/introduction-to-vfio/</guid>
      <description>Virtual Function I/O (VFIO) #  Introduced to replace the old-fashioned KVM PCI device assignment (virtio). Userspace driver interface Use IOMMU (AMD IOMMU, Intel VT-d, etc) Full PCI interrupt, MMIO and I/O port access, PCI configuration space access support Take an abstract view of a device: to support anything!  VFIO Device Filer descriptor #   located in /dev/vfio
     Each divided into regions
 Each region maps to a device resource (MMIO BAR, IO BAR, PCI configuration space) Region count and information discovered through ioctl.</description>
    </item>
    
    <item>
      <title>GPU Architecture Overview</title>
      <link>/2017-04-27/gpu-architecture-overview/</link>
      <pubDate>Thu, 27 Apr 2017 10:15:41 +0900</pubDate>
      
      <guid>/2017-04-27/gpu-architecture-overview/</guid>
      <description>GPU Model #    {: .center-image width:600px}
It explains several important designs that recent GPUs have adopted.
  MMIO.
 The CPU communicates with the GPU via MMIO. Hardware engines for DMA are supported for transferring large amounts of data, however, commands should be written via MMIO. The I/O ports can be used to indirectly access the MMIO regions, but rarely used. An open source device driver Nouveau currently never uses it.</description>
    </item>
    
    <item>
      <title>Hooking an SGX ENCLS Leaf Function Call from KVM</title>
      <link>/2017-04-21/hooking-an-sgx-encls-leaf-function-call-from-kvm/</link>
      <pubDate>Fri, 21 Apr 2017 20:36:41 +0900</pubDate>
      
      <guid>/2017-04-21/hooking-an-sgx-encls-leaf-function-call-from-kvm/</guid>
      <description>Environment #  Host: Ubuntu 14.04.5 LTS, Linux kernel 4.6.0, Intel Core-i7 6700 Skylake processor Guest: Ubuntu 14.04.4 LTS, Linux kernel 3.16.5, QEMU-KVM based virtual machine (using Intel VT-x)  1. ENCLS #   SGX Programming Reference, Section 5.2.1
  ENCLS instruction is used to execute an enclave system function (privileged) of specified leaf number.
  Software specifies the leaf function by setting the appropriate value in the register EAX as input.</description>
    </item>
    
    <item>
      <title>Intel SGX Instructions in Enclave Initialization</title>
      <link>/2017-04-05/intel-sgx-instructions-in-enclave-initialization/</link>
      <pubDate>Wed, 05 Apr 2017 22:13:25 +0900</pubDate>
      
      <guid>/2017-04-05/intel-sgx-instructions-in-enclave-initialization/</guid>
      <description>1. ECREATE #  [Intel SGX Explained p63] Section 5.3.1. Creation [Programming References p21] Section 5.3. ECREATE  An enclave is born when the system software issues the ECREATE instruction, which turns a free EPC page into the SECS for the new enclave.
ECREATE copies an SECS structure outside the EPC into an SECS page inside the EPC. The internal structure of SECS is not accessible to software.
Software sets the following fields in the source structure: SECS:BASEADDR, SECS:SIZE, and ATTRIBUTES.</description>
    </item>
    
    <item>
      <title>Intel SGX Protection Mechanism</title>
      <link>/2017-04-03/intel-sgx-protection-mechanism/</link>
      <pubDate>Mon, 03 Apr 2017 14:09:50 +0900</pubDate>
      
      <guid>/2017-04-03/intel-sgx-protection-mechanism/</guid>
      <description>All Figure numbers are same with those in the paper.
Glossary #  PMH: Page Miss Handler. MMU: Memory Management Unit. TLB: Translation Look-aside Buffer. FSM: Finite State Machine. EPC: Enclave Page Cache. EPCM: Enclave Page Cache Map. PRM: Processor Reserved Memory. ELRANGE: Enclave Linear Address Range.  Address Translation # Concepts #  Section 2.5.1 Address Translation Concepts
 System software relies on the CPU&amp;rsquo;s address translation mechanism for implementing isolation among less privileged pieces of software.</description>
    </item>
    
    <item>
      <title>PCI Express I/O System</title>
      <link>/2017-04-03/pci-express-i/o-system/</link>
      <pubDate>Mon, 03 Apr 2017 12:12:58 +0900</pubDate>
      
      <guid>/2017-04-03/pci-express-i/o-system/</guid>
      <description>I/O Hardware Overview # The basic I/O hardware elements, such as ports, buses, and device controllers, accomodate a wide variety of I/O devices.
To encapsulate the details and oddities of different devices, the kernel of an operating system is structured to use device-driver modules.
A device communicates with a computer system by sending signals over a cable or through the air.
 The device communicates with the machine via a connection point, or port.</description>
    </item>
    
  </channel>
</rss>
