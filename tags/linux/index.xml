<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>linux on Better Tomorrow with Computer Science</title>
    <link>/tags/linux/</link>
    <description>Recent content in linux on Better Tomorrow with Computer Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 16 Mar 2022 23:37:00 -0500</lastBuildDate><atom:link href="/tags/linux/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using RDMA CM</title>
      <link>/2022-03-16/using-rdma-cm/</link>
      <pubDate>Wed, 16 Mar 2022 23:37:00 -0500</pubDate>
      
      <guid>/2022-03-16/using-rdma-cm/</guid>
      <description>Years before, I posted how to use libibverbs for RDMA communication.
When initializing queue pair connection, we need some destination information:
bool changeQueuePairStateToRTR(struct ibv_qp* queue_pair, int ib_port, uint32_t destination_qp_number, uint16_t destination_local_id) { struct ibv_qp_attr rtr_attr; memset(&amp;amp;rtr_attr, 0, sizeof(rtr_attr)); rtr_attr.qp_state = ibv_qp_state::IBV_QPS_RTR; rtr_attr.path_mtu = ibv_mtu::IBV_MTU_1024; rtr_attr.rq_psn = 0; rtr_attr.max_dest_rd_atomic = 1; rtr_attr.min_rnr_timer = 0x12; rtr_attr.ah_attr.is_global = 0; rtr_attr.ah_attr.sl = 0; rtr_attr.ah_attr.src_path_bits = 0; rtr_attr.ah_attr.port_num = ib_port; rtr_attr.dest_qp_num = destination_qp_number; // here  rtr_attr.</description>
    </item>
    
    <item>
      <title>Libvirt VM Network Accessibility</title>
      <link>/2021-05-03/libvirt-vm-network-accessibility/</link>
      <pubDate>Mon, 03 May 2021 16:47:00 +0900</pubDate>
      
      <guid>/2021-05-03/libvirt-vm-network-accessibility/</guid>
      <description>Libvirt provides virtual networking 1 to VMs with a virtual network switch, which is implemented with a bridge by default:
$ ip addr ... 20: virbr0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 52:54:00:e9:7b:57 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever libvirt provides several modes for this bridge, two of which will be introduced in this post so that we can directly access to VMs via networking: NAT mode + port forwarding, and routed mode.</description>
    </item>
    
    <item>
      <title>Using Intel IOAT DMA</title>
      <link>/2021-04-26/using-intel-ioat-dma/</link>
      <pubDate>Mon, 26 Apr 2021 09:02:00 +0900</pubDate>
      
      <guid>/2021-04-26/using-intel-ioat-dma/</guid>
      <description>I/OAT (I/O Acceleration Technology) 1 # Intel I/OAT is a set of technologies for improving I/O performance. This post specifically illustrates how to use Intel QuickData Technology, which enables data copy by the chipset instead of the CPU, to move data more efficiently and provide fast throughput.
Using Linux DMA Engine # I/OAT (specifically QuickData Technology) is implemented as ioatdma kernel module in Linux, and integrated into the Linux DMA subsystem.</description>
    </item>
    
    <item>
      <title>Testing Ceph RBD Performance with Virtualization</title>
      <link>/2021-03-24/testing-ceph-rbd-performance-with-virtualization/</link>
      <pubDate>Wed, 24 Mar 2021 11:20:00 +0900</pubDate>
      
      <guid>/2021-03-24/testing-ceph-rbd-performance-with-virtualization/</guid>
      <description>This post explains how I measured Ceph RBD performance with block/network virtualization technology (virtio and vhost), and the result.
VM execution is done through qemu-system-x86_64, without using libvirt.
 Host: Fedora 33 (Linux kernel 5.10.19) Guest: Ubuntu 20.04 LTS Server (Linux kernel 5.4.0.67)  # Qemu general configuration qemu-system-x86_64 --machine q35,accel=kvm, --enable-kvm \ -m 2048 -smp 2 -cpu host \ -vnc :0 \ -hda disk.qcow2 \ &amp;lt;append configuration-specific options&amp;gt; Performance is measured with fio running on the guest VM with the following configuration file:</description>
    </item>
    
    <item>
      <title>Virtio and Vhost Architecture - Part 2</title>
      <link>/2021-03-15/virtio-and-vhost-architecture-part-2/</link>
      <pubDate>Mon, 15 Mar 2021 12:24:00 +0900</pubDate>
      
      <guid>/2021-03-15/virtio-and-vhost-architecture-part-2/</guid>
      <description>This post explains overheads of virtio, and introduce vhost that increases performance of virtio.
Virtio Review &amp;amp; Vhost Introduction #  virtio-net example diagram. [Source]   VirtI/O is implemented with virtqueues, shared by the guest and QEMU process. When the guest rings a doorbell after inserting requests into the virtqueue, the context is forwarded to host KVM handler (VM-exit), and again to QEMU process via ioeventfd. QEMU process reads the request from the shared virtqueue, and handles it.</description>
    </item>
    
    <item>
      <title>Virtio and Vhost Architecture - Part 1</title>
      <link>/2021-03-10/virtio-and-vhost-architecture-part-1/</link>
      <pubDate>Wed, 10 Mar 2021 17:01:00 +0900</pubDate>
      
      <guid>/2021-03-10/virtio-and-vhost-architecture-part-1/</guid>
      <description>This post explains virtio and vhost, device virtualization techniques used in Linux kernel virtual machine (KVM). Here, I focus only on block device and network device virtualization.
Virtio (Virtual I/O) # Virtio is one of I/O (block, NIC, etc) virtualization techniques that are used in virtualization. It is a paravirtualized I/O solution that implements a set of communication framework for I/O interaction between guest applications and hypervisor 1 2, which means a device driver that is aware of virtualization is required.</description>
    </item>
    
    <item>
      <title>Introduction to Flatpak</title>
      <link>/2020-08-27/introduction-to-flatpak/</link>
      <pubDate>Thu, 27 Aug 2020 17:22:00 +0900</pubDate>
      
      <guid>/2020-08-27/introduction-to-flatpak/</guid>
      <description>Flatpak is one of app sandboxing frameworks, along with AppImage and Snap 1. Although Snap is the most famous one, I think the internal architecture of Flatpak is more reliable.
Fedora Silverblue and EndlessOS provide software installation primarily via Flathub, a central repository of Flatpak based applications 2 3.
This post briefly summarizes how to use Flatpak in terms of implementing a sample applications.
Installing Flatpak # In Ubuntu distributions, there is no Flatpak preinstalled, while it is in Fedora.</description>
    </item>
    
    <item>
      <title>Introduction to Fedora Silverblue</title>
      <link>/2020-07-15/fedora-silverblue/</link>
      <pubDate>Wed, 15 Jul 2020 12:35:00 +0900</pubDate>
      
      <guid>/2020-07-15/fedora-silverblue/</guid>
      <description>Fedora Silverblue #    Fedora Silverblue 1 is an immutable desktop operating system based on Fedora Linux distribution. What immutable does mean is that most directories including rootfs (/) are mounted as read-only, and user applications run in an isolated execution environment. It is a part of Atomic Host project, and share the same underlying system with Fedora CoreOS (FCOS).
For this purpose, Fedora Silverblue adopted two technologies:</description>
    </item>
    
    <item>
      <title>Introduction to Programming Infiniband RDMA</title>
      <link>/2020-02-09/introduction-to-programming-infiniband/</link>
      <pubDate>Sun, 09 Feb 2020 19:58:00 +0900</pubDate>
      
      <guid>/2020-02-09/introduction-to-programming-infiniband/</guid>
      <description>This post explains the basic of RDMA programming. There are many examples and posts regarding this, however, I personally could not find enough explanations for the examples. It was hard to understand how it works, and here I summarize what I got.
Backgrounds # Channel Adapter (CA) # Channel adapter refers an end node in the infiniband network. It is equivalent of Ethernet network interface card (NIC), but with more features regarding Infiniband and RDMA 1.</description>
    </item>
    
    <item>
      <title>Building Mellanox OFED from source code</title>
      <link>/2020-01-25/building-mellanox-ofed-from-source/</link>
      <pubDate>Sat, 25 Jan 2020 10:28:00 +0900</pubDate>
      
      <guid>/2020-01-25/building-mellanox-ofed-from-source/</guid>
      <description>Mellanox is a manufacturer of networking products based on infiniband, which in these days are used for Remote DMA (RDMA). Though their documents are explained and managed well in their [website], I cannot find how to build an infiniband device driver from source code they provide.
Building Mellanox OFED source code: inside install script # Source code can be downloaded in [here]. Currently the latest version of MLNX_OFED is 4.7-3.2.9.0.</description>
    </item>
    
    <item>
      <title>Installing Kubernetes and cri-o in Debian</title>
      <link>/2019-11-21/installing-kubernetes-and-crio-in-debian/</link>
      <pubDate>Thu, 21 Nov 2019 18:32:00 +0900</pubDate>
      
      <guid>/2019-11-21/installing-kubernetes-and-crio-in-debian/</guid>
      <description>This post summarizes how to install cri-o container runtime and initialize a Kubernetes master node in Debian machine. Tested with Debian 10 running on a VirtualBox VM.
root@kubernetesdebian:/etc# cat os-release PRETTY_NAME=&amp;#34;Debian GNU/Linux 10 (buster)&amp;#34; NAME=&amp;#34;Debian GNU/Linux&amp;#34; VERSION_ID=&amp;#34;10&amp;#34; VERSION=&amp;#34;10 (buster)&amp;#34; VERSION_CODENAME=buster ID=debian HOME_URL=&amp;#34;https://www.debian.org/&amp;#34; SUPPORT_URL=&amp;#34;https://www.debian.org/support&amp;#34; BUG_REPORT_URL=&amp;#34;https://bugs.debian.org/&amp;#34; Installing cri-o # 0. Prerequiste for using cri-o with Kubernetes # Kubernetes requires the following configurations be set before using cri-o container runtime [link]:
modprobe overlay modprobe br_netfilter cat &amp;gt; /etc/sysctl.</description>
    </item>
    
    <item>
      <title>Code Server: Using vscode via Web Browsers</title>
      <link>/2019-11-10/code-server/</link>
      <pubDate>Sun, 10 Nov 2019 14:05:00 +0900</pubDate>
      
      <guid>/2019-11-10/code-server/</guid>
      <description>vscode running as a standalone app (lower right), and vscode frontend UI running in Safari web browser (upper left). Their looks are nearly identical except for menu, etc.
Visual Studio Code # Visual Studio Code, implemented and managed by Microsoft, is one of the best open-source code editors over the world. I&amp;rsquo;am using this too for almost every works; programming codes, writing Markdowns, writing Latex, etc. With tremendous number of plugins, its functionality is nearly limitless.</description>
    </item>
    
    <item>
      <title>Open Container Initiative (OCI) Standard, Image Spec</title>
      <link>/2019-10-10/open-container-initiative-image-spec/</link>
      <pubDate>Thu, 10 Oct 2019 20:35:00 +0900</pubDate>
      
      <guid>/2019-10-10/open-container-initiative-image-spec/</guid>
      <description>The Open Container Initiative (OCI) standard is an open standard for Linux containers. As born in 2013, Docker has been a de-facto standard of Linux container framework, but the OCI standard was born for a need of open standard, based on the Docker manifest. As the standard is based on Docker manifest, its specifications and structures are very similar to Dockers&#39;, enabling providing compatibilities between Docker and OCI-based container frameworks.</description>
    </item>
    
    <item>
      <title>Implementing a New Custom Netlink Family Protocol</title>
      <link>/2019-02-07/implementing-a-new-custom-netlink-family-protocol/</link>
      <pubDate>Thu, 07 Feb 2019 17:00:00 +0900</pubDate>
      
      <guid>/2019-02-07/implementing-a-new-custom-netlink-family-protocol/</guid>
      <description>Netlink Protocol # Netlink is a communication protocol between kernel and userspace. Unlike ioctl(), netlink is based on socket, which enables notification from the kernel to userspace. With ioctl(), the kernel can only send a response regarding to a user request. With netlink socket, however, user processes can be blocked via blocking functions such as recv() to receive any messages from the kernel.
#include &amp;lt;asm/types.h&amp;gt;#include &amp;lt;sys/socket.h&amp;gt;#include &amp;lt;linux/netlink.h&amp;gt; netlink_socket = socket (AF_NETLINK, socket_type, netlink_family); There are some predefined famous netlink protocol family: for instance, NETLINK_ROUTE for routing and link updates, NETLINK_KOBJECT_UEVENT for device events, and so on.</description>
    </item>
    
    <item>
      <title>udev: Function Flow for KOBJECT_UEVENT kernel group message </title>
      <link>/2018-11-28/udev-function-flow-for-kobject_uevent-kernel-group-message/</link>
      <pubDate>Wed, 28 Nov 2018 14:03:15 +0900</pubDate>
      
      <guid>/2018-11-28/udev-function-flow-for-kobject_uevent-kernel-group-message/</guid>
      <description>Identifying the device #    {: .center-image} [source]
When a USB device is inserted to system, the very first initialization function to be started is drivers/usb/core/usb.c:usb_init(), written in [here]. The USB root hub driver (i.e. hcd) initiates the USB device initialization, the USB core takes the control and initializes an actual device structure struct usb_device.
linux/include/linux/usb.h struct usb_device { int devnum; char devpath[16]; ... struct usb_device *parent; struct usb_bus *bus; struct usb_host_endpoint ep0; struct device dev; .</description>
    </item>
    
    <item>
      <title>udev: Device Manager for the Linux Kernel in Userspace</title>
      <link>/2018-11-27/udev-device-manager-for-the-linux-kernel-in-userspace/</link>
      <pubDate>Tue, 27 Nov 2018 10:05:14 +0900</pubDate>
      
      <guid>/2018-11-27/udev-device-manager-for-the-linux-kernel-in-userspace/</guid>
      <description>What is udev? #  udev (userspace /dev) is a device manager for the Linux kernel. As the successor of devfsd and hotplug, udev primaily manages device nodes in the /dev directory. At the same time, udev also handls all user space events raised when hardware devices are added into the system or removed from it, including firmware loading as reuqired by certain devices.
https://en.wikipedia.org/wiki/Udev
 udev first appeared at Linux kernel version 2.</description>
    </item>
    
    <item>
      <title>systemd Boot Process</title>
      <link>/2018-11-22/systemd-boot-process/</link>
      <pubDate>Thu, 22 Nov 2018 13:49:40 +0900</pubDate>
      
      <guid>/2018-11-22/systemd-boot-process/</guid>
      <description>What is systemd? #  systemd is a suite of basic building blocks for a Linux system. It provides a system and service manager that runs as PID 1 and starts the rest of the system.
https://www.freedesktop.org/wiki/Software/systemd
 systemd is now the init process running as PID 1 as indicated above. /sbin/init was the actual init process of Linux (also known as System V init boot system), it is now replaced with /usr/lib/systemd in many Linux distributions.</description>
    </item>
    
    <item>
      <title>Usermode Helper API</title>
      <link>/2017-05-10/usermode-helper-api/</link>
      <pubDate>Wed, 10 May 2017 01:58:06 +0900</pubDate>
      
      <guid>/2017-05-10/usermode-helper-api/</guid>
      <description>We already know that fork() and exec() are system calls for making a new process from user space.
However, system calls cannot be called in kernel space. Then how to execute a process from kernel space?
Usermode Helper API is for creating a user mode process from kernel space.
Data structure that is used for the API is struct subprocess_info.
/linux/include/kmod.h struct subprocess_info { struct work_struct work; struct completion* complete; const char* path; char** argv; char** envp; int wait; int retval; int (*init)(struct subprocess_info* info, struct cred* new); void (*cleanup)(struct subprocess_info* info); void* data; }; Simple example from the reference:</description>
    </item>
    
    <item>
      <title>CPU Affinity</title>
      <link>/2017-05-09/cpu-affinity/</link>
      <pubDate>Tue, 09 May 2017 21:56:25 +0900</pubDate>
      
      <guid>/2017-05-09/cpu-affinity/</guid>
      <description>CPU affinity, also called CPU pinning, enables the binding of a process or a thread to a specific CPU core, or CPU cores.
The following function is provided as a standard library to set affinity. [reference]
#include &amp;lt;sched.h&amp;gt; int sched_setaffinity(pid_t pid, size_t cpusetsize, cpu_set_t *mask); int sched_getaffinity(pid_t pid, size_t cpusetsize, cpu_set_t *mask); On success, schedu_setaffinity() and sched_getaffinity() return 0. On error, -1 is returned, and errno is set appropriately.
cpu_set_t can be set by defined macros as follows.</description>
    </item>
    
    <item>
      <title>Calling Kernel Module&#39;s Function from The Other Kernel Module</title>
      <link>/2017-05-03/calling-kernel-modules-function-from-the-other-kernel-module/</link>
      <pubDate>Wed, 03 May 2017 15:18:17 +0900</pubDate>
      
      <guid>/2017-05-03/calling-kernel-modules-function-from-the-other-kernel-module/</guid>
      <description>I made two kernel modules, one of which calls a function of the other module, but it kept saying me that WARNING: &amp;quot;&amp;lt;function_name&amp;gt;&amp;quot; undefined!. Even though I exported the function, there actually is another step that I should follow.
References: http://stackoverflow.com/a/9499893
What I did before finding the reference was to export the target function.
/kernel1/functions.h void function1(void); /kernel1/functions.c #include &amp;lt;linux/module.h&amp;gt;void function1(void){}; EXPORT_SYMBOL(function1); However, the other kernel module (say it kernel2) does not know where function1() exists.</description>
    </item>
    
    <item>
      <title>Introduction to VFIO</title>
      <link>/2017-04-27/introduction-to-vfio/</link>
      <pubDate>Thu, 27 Apr 2017 13:14:24 +0900</pubDate>
      
      <guid>/2017-04-27/introduction-to-vfio/</guid>
      <description>Virtual Function I/O (VFIO) #  Introduced to replace the old-fashioned KVM PCI device assignment (virtio). Userspace driver interface Use IOMMU (AMD IOMMU, Intel VT-d, etc) Full PCI interrupt, MMIO and I/O port access, PCI configuration space access support Take an abstract view of a device: to support anything!  VFIO Device Filer descriptor #   located in /dev/vfio
     Each divided into regions
 Each region maps to a device resource (MMIO BAR, IO BAR, PCI configuration space) Region count and information discovered through ioctl.</description>
    </item>
    
    <item>
      <title>Hooking an SGX ENCLS Leaf Function Call from KVM</title>
      <link>/2017-04-21/hooking-an-sgx-encls-leaf-function-call-from-kvm/</link>
      <pubDate>Fri, 21 Apr 2017 20:36:41 +0900</pubDate>
      
      <guid>/2017-04-21/hooking-an-sgx-encls-leaf-function-call-from-kvm/</guid>
      <description>Environment #  Host: Ubuntu 14.04.5 LTS, Linux kernel 4.6.0, Intel Core-i7 6700 Skylake processor Guest: Ubuntu 14.04.4 LTS, Linux kernel 3.16.5, QEMU-KVM based virtual machine (using Intel VT-x)  1. ENCLS #   SGX Programming Reference, Section 5.2.1
  ENCLS instruction is used to execute an enclave system function (privileged) of specified leaf number.
  Software specifies the leaf function by setting the appropriate value in the register EAX as input.</description>
    </item>
    
  </channel>
</rss>
