<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>parallelism on Better Tomorrow with Computer Science</title>
    <link>/tags/parallelism/</link>
    <description>Recent content in parallelism on Better Tomorrow with Computer Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 20 Sep 2024 10:32:00 -0400</lastBuildDate><atom:link href="/tags/parallelism/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introducing Context Parallelism</title>
      <link>/2024-09-20/introducing-context-parallelism/</link>
      <pubDate>Fri, 20 Sep 2024 10:32:00 -0400</pubDate>
      
      <guid>/2024-09-20/introducing-context-parallelism/</guid>
      <description>In the previous analysis of sequence parallelism, I covered two papers 1 2. Those are early works about sequence parallelism and didn&amp;rsquo;t get attention as there was low demand for context parallelism. After LLMs are required to have longer context support, new papers that tackle the problems of such early works have been emerged.
What are the Problems of Early Sequence Parallelism Works? #    Both works follow the traditional attention computation: compute QK^T, apply mask.</description>
    </item>
    
    <item>
      <title>Flash Attention</title>
      <link>/2024-01-21/flash-attention/</link>
      <pubDate>Sun, 21 Jan 2024 11:20:00 -0500</pubDate>
      
      <guid>/2024-01-21/flash-attention/</guid>
      <description>This post explains flash attention 1 2.
More references are also useful to understand flash attention as well 3 4 5.
Backgrounds # Attention # $$\text{Attention}(Q, K, V)=\text{softmax}(\frac{QK^T}{\sqrt{d^k}})V$$
This equation can be implemented as:
class OPTAttention(nn.Module): def forward(...): # hidden states is an input tenor of Attention layer # Calculate Q, K, and V with linear projections to the input # query_states = self.q_proj(hidden_states) # key_states = self.k_proj(hidden_states) # value_states = self.</description>
    </item>
    
    <item>
      <title>Tensor Parallelism and Sequence Parallelism: Detailed Analysis</title>
      <link>/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/</link>
      <pubDate>Thu, 11 Jan 2024 18:20:00 -0500</pubDate>
      
      <guid>/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/</guid>
      <description>This post explains how tensor model parallelism and sequence parallelism work especially on attention layer, and how they are different.
Backgrounds # Attention Layer #    Attention calculation with a single sequence with T number of tokens. d_attn is config.embed_dim // config.num_attention_heads Bold boxes are model parameters, while others are temporarily created tensors. All the other terms are borrowed from HuggingFace Transformers OPT config. Implementation of computing attention:</description>
    </item>
    
    <item>
      <title>Parallelism in Distributed Deep Learning</title>
      <link>/2022-06-11/parallelism-in-distributed-deep-learning/</link>
      <pubDate>Sat, 11 Jun 2022 19:33:00 -0400</pubDate>
      
      <guid>/2022-06-11/parallelism-in-distributed-deep-learning/</guid>
      <description>Distributed deep learning refers to use a distributed system that includes several workers to perform inference or training deep learning. Since mid 2010, people have been thinking about accelerating deep learning with scale-out, and distributd deep learning has been introduced. Parameter server is one of the well-known architecture for distributed deep learning.
Recent days, many parallelization mechanisms, the way of distributing computation to multiple workers, have been introduced. First one was to split batches into several microbatches and distributed it, namely data parallelism, which the parameter server architecture is for.</description>
    </item>
    
  </channel>
</rss>
