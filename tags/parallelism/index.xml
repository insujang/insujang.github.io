<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>parallelism on Better Tomorrow with Computer Science</title>
    <link>/tags/parallelism/</link>
    <description>Recent content in parallelism on Better Tomorrow with Computer Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 21 Jan 2024 11:20:00 -0500</lastBuildDate><atom:link href="/tags/parallelism/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flash Attention</title>
      <link>/2024-01-21/flash-attention/</link>
      <pubDate>Sun, 21 Jan 2024 11:20:00 -0500</pubDate>
      
      <guid>/2024-01-21/flash-attention/</guid>
      <description>This post explains flash attention 1 2.
More references are also useful to understand flash attention as well 3 4 5.
Backgrounds # Attention # $$\text{Attention}(Q, K, V)=\text{softmax}(\frac{QK^T}{\sqrt{d^k}})V$$
This equation can be implemented as:
class OPTAttention(nn.Module): def forward(...): # hidden states is an input tenor of Attention layer # Calculate Q, K, and V with linear projections to the input # query_states = self.q_proj(hidden_states) # key_states = self.k_proj(hidden_states) # value_states = self.</description>
    </item>
    
    <item>
      <title>Tensor Parallelism and Sequence Parallelism: Detailed Analysis</title>
      <link>/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/</link>
      <pubDate>Thu, 11 Jan 2024 18:20:00 -0500</pubDate>
      
      <guid>/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/</guid>
      <description>This post explains how tensor model parallelism and sequence parallelism work especially on attention layer, and how they are different.
Backgrounds # Attention Layer #    Attention calculation with a single sequence with T number of tokens. d_attn is config.embed_dim // config.num_attention_heads Bold boxes are model parameters, while others are temporarily created tensors. All the other terms are borrowed from HuggingFace Transformers OPT config. Implementation of computing attention:</description>
    </item>
    
    <item>
      <title>Parallelism in Distributed Deep Learning</title>
      <link>/2022-06-11/parallelism-in-distributed-deep-learning/</link>
      <pubDate>Sat, 11 Jun 2022 19:33:00 -0400</pubDate>
      
      <guid>/2022-06-11/parallelism-in-distributed-deep-learning/</guid>
      <description>Distributed deep learning refers to use a distributed system that includes several workers to perform inference or training deep learning. Since mid 2010, people have been thinking about accelerating deep learning with scale-out, and distributd deep learning has been introduced. Parameter server is one of the well-known architecture for distributed deep learning.
Recent days, many parallelization mechanisms, the way of distributing computation to multiple workers, have been introduced. First one was to split batches into several microbatches and distributed it, namely data parallelism, which the parameter server architecture is for.</description>
    </item>
    
  </channel>
</rss>
