<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dl on Better Tomorrow with Computer Science</title>
    <link>/tags/dl/</link>
    <description>Recent content in dl on Better Tomorrow with Computer Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 20 Sep 2024 10:32:00 -0400</lastBuildDate><atom:link href="/tags/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introducing Context Parallelism</title>
      <link>/2024-09-20/introducing-context-parallelism/</link>
      <pubDate>Fri, 20 Sep 2024 10:32:00 -0400</pubDate>
      
      <guid>/2024-09-20/introducing-context-parallelism/</guid>
      <description>In the previous analysis of sequence parallelism, I covered two papers 1 2. Those are early works about sequence parallelism and didn&amp;rsquo;t get attention as there was low demand for context parallelism. After LLMs are required to have longer context support, new papers that tackle the problems of such early works have been emerged.
What are the Problems of Early Sequence Parallelism Works? #    Both works follow the traditional attention computation: compute QK^T, apply mask.</description>
    </item>
    
    <item>
      <title>Flash Attention</title>
      <link>/2024-01-21/flash-attention/</link>
      <pubDate>Sun, 21 Jan 2024 11:20:00 -0500</pubDate>
      
      <guid>/2024-01-21/flash-attention/</guid>
      <description>This post explains flash attention 1 2.
More references are also useful to understand flash attention as well 3 4 5.
Backgrounds # Attention # $$\text{Attention}(Q, K, V)=\text{softmax}(\frac{QK^T}{\sqrt{d^k}})V$$
This equation can be implemented as:
class OPTAttention(nn.Module): def forward(...): # hidden states is an input tenor of Attention layer # Calculate Q, K, and V with linear projections to the input # query_states = self.q_proj(hidden_states) # key_states = self.k_proj(hidden_states) # value_states = self.</description>
    </item>
    
    <item>
      <title>Tensor Parallelism and Sequence Parallelism: Detailed Analysis</title>
      <link>/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/</link>
      <pubDate>Thu, 11 Jan 2024 18:20:00 -0500</pubDate>
      
      <guid>/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/</guid>
      <description>This post explains how tensor model parallelism and sequence parallelism work especially on attention layer, and how they are different.
Backgrounds # Attention Layer #    Attention calculation with a single sequence with T number of tokens. d_attn is config.embed_dim // config.num_attention_heads Bold boxes are model parameters, while others are temporarily created tensors. All the other terms are borrowed from HuggingFace Transformers OPT config. Implementation of computing attention:</description>
    </item>
    
    <item>
      <title>LLM Inference: Continuous Batching and PagedAttention</title>
      <link>/2024-01-07/llm-inference-continuous-batching-and-pagedattention/</link>
      <pubDate>Sun, 07 Jan 2024 20:20:00 -0500</pubDate>
      
      <guid>/2024-01-07/llm-inference-continuous-batching-and-pagedattention/</guid>
      <description>Recent days, many papers have been published to optimize LLM inference. This post introduces two of them, which focus on improving throughput by exploiting characteristics of batched LLM serving and characteristics of attention.
Orca # Orca, published in OSDI&#39;22, proposes two novel techniques: 1. continuous batcing (or iteration-level scheduling) 1, and 2. selective batching.
Continuous Batching # Before the introduction of continuous batching, static batching starts batch at once and wait all batch to complete their computation.</description>
    </item>
    
    <item>
      <title>LLM Inference: Autoregressive Generation and Attention KV Cache</title>
      <link>/2024-01-07/llm-inference-autoregressive-generation-and-attention-kv-cache/</link>
      <pubDate>Sun, 07 Jan 2024 18:20:00 -0500</pubDate>
      
      <guid>/2024-01-07/llm-inference-autoregressive-generation-and-attention-kv-cache/</guid>
      <description>This post explains the basic of LLM inference, mainly focusing on differences from training LLM.
Autreogressive Text Generation # Unlike training, where tokens are parallelized and trained, inference generates tokens one by one. Therefore, to create a full sentence, several forward pass should be executed (# tokens times). The following video from HuggingFace illustrates how it works.
    Autoregressive token generation. Source: HuggingFace
 Before generating the first token, LLM first puts all input tokens to generate context.</description>
    </item>
    
    <item>
      <title>Torch FX Transformation and Pipeline Parallelism</title>
      <link>/2023-04-22/torch-fx-transformation-and-pipeline-parallelism/</link>
      <pubDate>Sat, 22 Apr 2023 11:13:00 -0400</pubDate>
      
      <guid>/2023-04-22/torch-fx-transformation-and-pipeline-parallelism/</guid>
      <description>Torch fx # torch.fx is a PyTorch module that captures a model and applies transformation for optimization 1. In recent days, the importance of model optimization is getting more important. torch.fx enables transparent transformation without touching to the original model implementation, allowing fine-grained model optimization.
 Since PyTorch 2.0, it seems TorchDynamo replaces legacy fx.tracer for tracing the model.   This post focuses on existing torch.fx module, and I will post another one regarding TorchDynamo if I have a chance.</description>
    </item>
    
    <item>
      <title>Using HuggingFace Transformers</title>
      <link>/2023-04-19/using-huggingface-transformers/</link>
      <pubDate>Wed, 19 Apr 2023 16:10:00 -0400</pubDate>
      
      <guid>/2023-04-19/using-huggingface-transformers/</guid>
      <description>HF Transformers # HuggingFace (ðŸ¤—) Transformers is a library that enables to easily download the state-of-the-art pretrained models. It is also possible to create and train a model from scratch, after modifying the structure of existing models. Although the library starts from transformer based language models, it became a general community hub and includes other models such as convolution based Resnet.
It can easily be installed via pip 1:
pip install transformers  Most code is borrowed from HuggingFace transformers example codes.</description>
    </item>
    
    <item>
      <title>Analyzing Parallelization of Attention</title>
      <link>/2022-08-03/analyzing-parallelization-of-attention/</link>
      <pubDate>Wed, 03 Aug 2022 14:30:00 -0400</pubDate>
      
      <guid>/2022-08-03/analyzing-parallelization-of-attention/</guid>
      <description>We exploit the inherent parallelism in the multi-head attention operation to partition the self-attention block (shown in Figure 5b). The key (K), Query (Q), and value (V) matrices can be partitioned in a column-parallel fashion. The output linear layer can then directly operate on the partitioned output of the attention operation (weight matrix partitioned across rows).
Deepak Narayanan et al, Efficient large-scale language model training on GPU clusters using megatron-LM, SC&#39;21</description>
    </item>
    
    <item>
      <title>Analysis of Transformer Model</title>
      <link>/2022-07-30/analysis-of-transformer-model/</link>
      <pubDate>Sat, 30 Jul 2022 23:48:00 -0400</pubDate>
      
      <guid>/2022-07-30/analysis-of-transformer-model/</guid>
      <description>This post analyzes transformer models, specifically memory and computation overhead.
Many transformer based models just explain themselves as a model with X-B parameters; I wanted to break it down and look into the model structure how they are stored and used in actual computing hardware.
 Many illustrations and analysis are based on the following papers 1.
 Transformer-based Model # Since Google announed attention and transformer models in 2017 2, MLP and image classification models are rapidly adopting transformer layers.</description>
    </item>
    
    <item>
      <title>Parallelism in Distributed Deep Learning</title>
      <link>/2022-06-11/parallelism-in-distributed-deep-learning/</link>
      <pubDate>Sat, 11 Jun 2022 19:33:00 -0400</pubDate>
      
      <guid>/2022-06-11/parallelism-in-distributed-deep-learning/</guid>
      <description>Distributed deep learning refers to use a distributed system that includes several workers to perform inference or training deep learning. Since mid 2010, people have been thinking about accelerating deep learning with scale-out, and distributd deep learning has been introduced. Parameter server is one of the well-known architecture for distributed deep learning.
Recent days, many parallelization mechanisms, the way of distributing computation to multiple workers, have been introduced. First one was to split batches into several microbatches and distributed it, namely data parallelism, which the parameter server architecture is for.</description>
    </item>
    
  </channel>
</rss>
